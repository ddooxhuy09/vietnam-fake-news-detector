{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Installation\n",
        "%pip install transformers==4.35.0\n",
        "%pip install torch torchvision torchaudio\n",
        "%pip install underthesea\n",
        "%pip install onnx onnxruntime\n",
        "%pip install optimum[onnxruntime]\n",
        "%pip install datasets\n",
        "%pip install accelerate -U\n",
        "%pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuO6qolHHiHy",
        "outputId": "4f3ee5ba-a6e8-457b-9bcb-fd11cd6dd7b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.35.0\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (2.32.4)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0)\n",
            "  Downloading tokenizers-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (1.2.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (2025.11.12)\n",
            "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.36.0 requires huggingface-hub<2.0,>=0.34.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "gradio 5.50.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 5.1.2 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\n",
            "datasets 4.0.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "gradio-client 1.14.0 requires huggingface-hub<2.0,>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "peft 0.18.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "accelerate 1.12.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.35.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-8.3.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from underthesea) (8.3.1)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.12/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from underthesea) (2.32.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.5.2)\n",
            "Requirement already satisfied: scikit-learn>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from underthesea) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from underthesea) (6.0.3)\n",
            "Collecting underthesea_core==1.0.5 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.5-cp312-cp312-manylinux2010_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from underthesea) (0.17.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.8->underthesea) (2025.11.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.1->underthesea) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (3.20.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->underthesea) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->underthesea) (2025.11.12)\n",
            "Downloading underthesea-8.3.0-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading underthesea_core-1.0.5-cp312-cp312-manylinux2010_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.4/978.4 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: underthesea_core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.11 underthesea-8.3.0 underthesea_core-1.0.5\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.20.0 onnxruntime-1.23.2\n",
            "Collecting optimum[onnxruntime]\n",
            "  Downloading optimum-2.0.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (4.35.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (2.9.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from optimum[onnxruntime]) (0.17.3)\n",
            "Collecting optimum-onnx[onnxruntime] (from optimum[onnxruntime])\n",
            "  Downloading optimum_onnx-0.0.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.20.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.7.0)\n",
            "Collecting transformers>=4.29 (from optimum[onnxruntime])\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.20.0)\n",
            "Requirement already satisfied: onnxruntime>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (1.23.2)\n",
            "Collecting huggingface_hub>=0.8.0 (from optimum[onnxruntime])\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.29->optimum[onnxruntime])\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[onnxruntime]) (1.2.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (5.29.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[onnxruntime]) (2025.11.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.18.0->optimum-onnx[onnxruntime]; extra == \"onnxruntime\"->optimum[onnxruntime]) (10.0)\n",
            "Downloading optimum-2.0.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum_onnx-0.0.3-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers, optimum, optimum-onnx\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.14.1\n",
            "    Uninstalling tokenizers-0.14.1:\n",
            "      Successfully uninstalled tokenizers-0.14.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.0\n",
            "    Uninstalling transformers-4.35.0:\n",
            "      Successfully uninstalled transformers-4.35.0\n",
            "Successfully installed huggingface_hub-0.36.0 optimum-2.0.0 optimum-onnx-0.0.3 tokenizers-0.21.4 transformers-4.55.4\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.13-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (5.29.5)\n",
            "Downloading onnxscript-0.5.7-py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.4/693.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.13-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx_ir, onnxscript\n",
            "Successfully installed onnx_ir-0.1.13 onnxscript-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 1: IMPORTS VÀ SETUP\n",
        "# ================================================================================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "import torch\n",
        "import unicodedata\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import json\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Check dependencies\n",
        "try:\n",
        "    from underthesea import word_tokenize\n",
        "    UNDERTHESEA_AVAILABLE = True\n",
        "    print(\"✓ underthesea available\")\n",
        "except ImportError:\n",
        "    UNDERTHESEA_AVAILABLE = False\n",
        "    print(\"⚠️ underthesea not available - word segmentation disabled\")\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✓ Device: {device}\")"
      ],
      "metadata": {
        "id": "Q0W4lbhsCmRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_PZyZ_-CkE9"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 2: LOAD DATA\n",
        "# ================================================================================\n",
        "\n",
        "# Mount Google Drive (nếu dùng Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your dataset\n",
        "# Thay đổi path theo file của bạn\n",
        "data_path = '/content/drive/MyDrive/FakeNewsModels/MERGED_DATASET_FINAL.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Kiểm tra data\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nSample data:\")\n",
        "print(df.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 3: TRAIN/VAL/TEST SPLIT (FIXED)\n",
        "# ================================================================================\n",
        "\n",
        "# Split data theo tỷ lệ: 70% train, 15% val, 15% test\n",
        "print(\"Splitting dataset into train/val/test...\")\n",
        "\n",
        "# Bước 1: Split train vs (val+test)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.30,  # 30% cho val+test\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# Bước 2: Split (val+test) thành val và test\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.50,  # 50% của 30% = 15% cho test\n",
        "    random_state=42,\n",
        "    stratify=temp_df['label']\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Dataset Split Summary\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total samples:  {len(df):,}\")\n",
        "print(f\"Train set:      {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Val set:        {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test set:       {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'Label':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
        "print(f\"{'-'*40}\")\n",
        "for label in sorted(df['label'].unique()):\n",
        "    train_count = (train_df['label'] == label).sum()\n",
        "    val_count = (val_df['label'] == label).sum()\n",
        "    test_count = (test_df['label'] == label).sum()\n",
        "    print(f\"{label:<10} {train_count:<10} {val_count:<10} {test_count:<10}\")\n",
        "\n",
        "# Reset index\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"\\n✓ Data split completed\")\n"
      ],
      "metadata": {
        "id": "SDEuZ4KPDNpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 4: TEXT NORMALIZER\n",
        "# ================================================================================\n",
        "\n",
        "class VietnameseTextNormalizer:\n",
        "    \"\"\"Normalizer tối giản cho PhoBERT\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.use_word_segment = UNDERTHESEA_AVAILABLE\n",
        "\n",
        "    def normalize_unicode(self, text: str) -> str:\n",
        "        \"\"\"Chuẩn hóa Unicode về NFC\"\"\"\n",
        "        return unicodedata.normalize('NFC', text)\n",
        "\n",
        "    def clean_special_chars(self, text: str) -> str:\n",
        "        \"\"\"Giữ chữ Việt + dấu câu cơ bản\"\"\"\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http[s]?://\\S+', ' ', text)\n",
        "\n",
        "        # Keep Vietnamese + punctuation\n",
        "        text = re.sub(\n",
        "            r'[^\\w\\s.,!?àáảãạăằắẳẵặâầấẩẫậèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵđĐ]',\n",
        "            ' ',\n",
        "            text\n",
        "        )\n",
        "\n",
        "        return text\n",
        "\n",
        "    def word_segment(self, text: str) -> str:\n",
        "        \"\"\"Tách từ cho PhoBERT\"\"\"\n",
        "        if not self.use_word_segment:\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            from underthesea import word_tokenize\n",
        "            text = word_tokenize(text, format=\"text\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return text\n",
        "\n",
        "    def normalize(self, text: Optional[str], preserve_mask: bool = False) -> str:\n",
        "        \"\"\"Pipeline đơn giản\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # 1. Unicode\n",
        "        text = self.normalize_unicode(text)\n",
        "\n",
        "        # 2. Clean\n",
        "        text = text.strip()\n",
        "        text = self.clean_special_chars(text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # 3. Word segment\n",
        "        if not preserve_mask:\n",
        "            text = self.word_segment(text)\n",
        "\n",
        "        # 4. Final cleanup\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "# Initialize normalizer\n",
        "normalizer = VietnameseTextNormalizer()\n",
        "print(\"✓ Text normalizer initialized\")\n"
      ],
      "metadata": {
        "id": "tZGvgV0dDZO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 5: SEMANTIC CHUNKER (FIXED VERSION)\n",
        "# ================================================================================\n",
        "\n",
        "class SemanticChunkRetriever:\n",
        "    \"\"\"\n",
        "    FIXED VERSION: Improved chunking với better handling of long sentences\n",
        "    \"\"\"\n",
        "    def __init__(self, chunk_size=400):\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def chunk_document(self, text):\n",
        "        \"\"\"\n",
        "        FIX: Thêm dấu '-' vào pattern và handle oversized chunks\n",
        "        \"\"\"\n",
        "        # Split by sentence-ending punctuation AND dash\n",
        "        sentences = re.split(r'[.!?\\-]\\s+', text)  # ← FIXED: Added '-'\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_len = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            if not sent.strip():\n",
        "                continue\n",
        "\n",
        "            # FIX: Hard limit để tránh oversized chunks\n",
        "            if current_len + len(sent) > self.chunk_size:\n",
        "                if current_chunk:  # Save current chunk\n",
        "                    chunks.append(' . '.join(current_chunk))\n",
        "\n",
        "                # FIX: Nếu sentence đơn lẻ quá dài, split by words\n",
        "                if len(sent) > self.chunk_size * 1.5:\n",
        "                    words = sent.split()\n",
        "                    temp_chunk = []\n",
        "                    temp_len = 0\n",
        "                    for word in words:\n",
        "                        if temp_len + len(word) > self.chunk_size:\n",
        "                            if temp_chunk:\n",
        "                                chunks.append(' '.join(temp_chunk))\n",
        "                            temp_chunk = [word]\n",
        "                            temp_len = len(word)\n",
        "                        else:\n",
        "                            temp_chunk.append(word)\n",
        "                            temp_len += len(word) + 1\n",
        "                    if temp_chunk:\n",
        "                        current_chunk = temp_chunk\n",
        "                        current_len = temp_len\n",
        "                else:\n",
        "                    current_chunk = [sent]\n",
        "                    current_len = len(sent)\n",
        "            else:\n",
        "                current_chunk.append(sent)\n",
        "                current_len += len(sent)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' . '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "# Initialize chunker\n",
        "chunker = SemanticChunkRetriever(chunk_size=400)\n",
        "print(\"✓ Semantic chunker initialized (FIXED)\")\n"
      ],
      "metadata": {
        "id": "Mdfzyu-bDg6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 6: LOAD MODELS (TOKENIZER & RETRIEVER)\n",
        "# ================================================================================\n",
        "\n",
        "print(\"Loading PhoBERT tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\n",
        "print(\"✓ PhoBERT tokenizer loaded\")\n",
        "\n",
        "print(\"\\nLoading Vietnamese-SBERT for RAG retrieval...\")\n",
        "retriever = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
        "retriever.eval()\n",
        "print(\"✓ Vietnamese-SBERT loaded\")\n"
      ],
      "metadata": {
        "id": "p9Pe-duZDiaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 7: CHUNK ATTENTION LAYER\n",
        "# ================================================================================\n",
        "\n",
        "class ChunkAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism ở chunk level\n",
        "    Input: [batch, num_chunks, hidden_size]\n",
        "    Output:\n",
        "        - context: [batch, hidden_size] - weighted sum của chunks\n",
        "        - attention_weights: [batch, num_chunks] - attention scores\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int, attention_hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, attention_hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(attention_hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, chunk_embeddings: torch.Tensor, mask: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            chunk_embeddings: [batch, num_chunks, hidden_size]\n",
        "            mask: [batch, num_chunks] - 1 for valid chunks, 0 for padding\n",
        "\n",
        "        Returns:\n",
        "            context: [batch, hidden_size]\n",
        "            attention_weights: [batch, num_chunks]\n",
        "        \"\"\"\n",
        "        # Calculate attention scores: [batch, num_chunks, 1]\n",
        "        scores = self.attention(chunk_embeddings)\n",
        "        scores = scores.squeeze(-1)  # [batch, num_chunks]\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=1)  # [batch, num_chunks]\n",
        "\n",
        "        # Weighted sum: [batch, hidden_size]\n",
        "        context = torch.bmm(\n",
        "            attention_weights.unsqueeze(1),  # [batch, 1, num_chunks]\n",
        "            chunk_embeddings                  # [batch, num_chunks, hidden_size]\n",
        "        ).squeeze(1)  # [batch, hidden_size]\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "print(\"✓ ChunkAttentionLayer defined\")\n"
      ],
      "metadata": {
        "id": "c8C1i4j8DkQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 8: DATASET (FIXED VERSION)\n",
        "# ================================================================================\n",
        "\n",
        "class HierarchicalAttentionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: No empty padding, better RAG retrieval\n",
        "\n",
        "    KEY FIXES:\n",
        "    1. No empty string padding - duplicate chunks instead\n",
        "    2. Minimum chunks validation\n",
        "    3. Similarity threshold for RAG\n",
        "    4. Chunk length validation before tokenization\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        normalizer: VietnameseTextNormalizer,\n",
        "        retriever: SentenceTransformer,\n",
        "        chunk_size: int = 400,\n",
        "        top_k: int = 5,\n",
        "        max_length: int = 256,\n",
        "        min_chunks: int = 3,  # NEW: Minimum chunks required\n",
        "        min_similarity: float = 0.15,  # NEW: Minimum similarity threshold\n",
        "        verbose: bool = False\n",
        "    ):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.normalizer = normalizer\n",
        "        self.retriever = retriever\n",
        "        self.chunk_size = chunk_size\n",
        "        self.top_k = top_k\n",
        "        self.max_length = max_length\n",
        "        self.min_chunks = min_chunks\n",
        "        self.min_similarity = min_similarity\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Helper chunker\n",
        "        self.chunker = SemanticChunkRetriever(chunk_size=chunk_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        row = self.df.iloc[idx]\n",
        "        label = int(row['label'])\n",
        "\n",
        "        # 1. Lấy Title và Content\n",
        "        title = str(row.get('title', ''))\n",
        "        content = str(row.get('content', ''))\n",
        "\n",
        "        # Normalize\n",
        "        title = self.normalizer.normalize(title)\n",
        "        content = self.normalizer.normalize(content)\n",
        "\n",
        "        # 2. Chunking Content\n",
        "        raw_chunks = self.chunker.chunk_document(content)\n",
        "\n",
        "        # ========================================\n",
        "        # FIX 1: VALIDATE MINIMUM CHUNKS\n",
        "        # ========================================\n",
        "        if len(raw_chunks) < self.min_chunks:\n",
        "            if self.verbose:\n",
        "                print(f\"⚠️ Sample {idx} only has {len(raw_chunks)} chunks < min {self.min_chunks}\")\n",
        "            # Duplicate chunks to reach minimum\n",
        "            while len(raw_chunks) < self.min_chunks:\n",
        "                raw_chunks.extend(raw_chunks[:self.min_chunks - len(raw_chunks)])\n",
        "\n",
        "        # 3. RAG LOGIC: Retrieve Top-K chunks\n",
        "        if len(raw_chunks) <= self.top_k:\n",
        "            selected_chunks = raw_chunks\n",
        "        else:\n",
        "            # Use title as query, fallback to first chunk\n",
        "            query = title if title.strip() else raw_chunks[0]\n",
        "\n",
        "            # Encode query and chunks\n",
        "            query_emb = self.retriever.encode(query, convert_to_tensor=True)\n",
        "            chunk_embs = self.retriever.encode(raw_chunks, convert_to_tensor=True)\n",
        "\n",
        "            # Compute similarities\n",
        "            similarities = F.cosine_similarity(\n",
        "                query_emb.unsqueeze(0),\n",
        "                chunk_embs,\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # ========================================\n",
        "            # FIX 2: FILTER LOW-SIMILARITY CHUNKS\n",
        "            # ========================================\n",
        "            valid_indices = (similarities >= self.min_similarity).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            if len(valid_indices) < self.top_k:\n",
        "                # Not enough valid chunks, take top-k anyway\n",
        "                top_indices = similarities.argsort(descending=True)[:self.top_k]\n",
        "            else:\n",
        "                # Sort valid indices by similarity and take top-k\n",
        "                valid_sims = similarities[valid_indices]\n",
        "                sorted_valid = valid_indices[valid_sims.argsort(descending=True)]\n",
        "                top_indices = sorted_valid[:self.top_k]\n",
        "\n",
        "            selected_chunks = [raw_chunks[i] for i in top_indices]\n",
        "\n",
        "        # ========================================\n",
        "        # FIX 3: NO EMPTY PADDING - DUPLICATE INSTEAD\n",
        "        # ========================================\n",
        "        while len(selected_chunks) < self.top_k:\n",
        "            # Duplicate existing chunks cyclically\n",
        "            selected_chunks.append(\n",
        "                selected_chunks[len(selected_chunks) % len(raw_chunks)]\n",
        "            )\n",
        "\n",
        "        # Truncate if over\n",
        "        selected_chunks = selected_chunks[:self.top_k]\n",
        "\n",
        "        # ========================================\n",
        "        # FIX 4: VALIDATE CHUNK LENGTHS\n",
        "        # ========================================\n",
        "        max_chars = self.max_length * 4  # Conservative: 4 chars per token\n",
        "        validated_chunks = []\n",
        "        for chunk in selected_chunks:\n",
        "            if len(chunk) > max_chars:\n",
        "                if self.verbose:\n",
        "                    print(f\"⚠️ Chunk too long ({len(chunk)} chars), truncating to {max_chars}\")\n",
        "                chunk = chunk[:max_chars]\n",
        "            validated_chunks.append(chunk)\n",
        "\n",
        "        # 4. Tokenize chunks\n",
        "        chunk_encodings = self.tokenizer(\n",
        "            validated_chunks,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'chunk_input_ids': chunk_encodings['input_ids'],\n",
        "            'chunk_attention_masks': chunk_encodings['attention_mask'],\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"✓ HierarchicalAttentionDataset defined (FIXED)\")\n"
      ],
      "metadata": {
        "id": "zAG-fWPzDle2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 9: HAN MODEL\n",
        "# ================================================================================\n",
        "\n",
        "class HierarchicalAttentionClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Attention Network cho phân loại fake news\n",
        "\n",
        "    Architecture:\n",
        "    1. PhoBERT encode từng chunk → chunk embeddings\n",
        "    2. Chunk-level attention → document representation\n",
        "    3. Classification head → logits\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        phobert_name: str = \"vinai/phobert-base-v2\",\n",
        "        chunk_attention_hidden: int = 128,\n",
        "        num_classes: int = 2,\n",
        "        dropout: float = 0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # PhoBERT encoder\n",
        "        self.phobert = AutoModel.from_pretrained(phobert_name)\n",
        "        self.hidden_size = self.phobert.config.hidden_size  # 768\n",
        "\n",
        "        # Chunk-level attention\n",
        "        self.chunk_attention = ChunkAttentionLayer(\n",
        "            hidden_size=self.hidden_size,\n",
        "            attention_hidden=chunk_attention_hidden\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def encode_chunks(\n",
        "        self,\n",
        "        chunk_input_ids: torch.Tensor,\n",
        "        chunk_attention_masks: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode từng chunk bằng PhoBERT\n",
        "\n",
        "        Args:\n",
        "            chunk_input_ids: [batch, num_chunks, max_length]\n",
        "            chunk_attention_masks: [batch, num_chunks, max_length]\n",
        "\n",
        "        Returns:\n",
        "            chunk_embeddings: [batch, num_chunks, hidden_size]\n",
        "        \"\"\"\n",
        "        batch_size, num_chunks, max_length = chunk_input_ids.shape\n",
        "\n",
        "        # Reshape to [batch * num_chunks, max_length]\n",
        "        flat_input_ids = chunk_input_ids.view(-1, max_length)\n",
        "        flat_attention_masks = chunk_attention_masks.view(-1, max_length)\n",
        "\n",
        "        # Encode with PhoBERT\n",
        "        outputs = self.phobert(\n",
        "            input_ids=flat_input_ids,\n",
        "            attention_mask=flat_attention_masks\n",
        "        )\n",
        "\n",
        "        # Get [CLS] token embeddings: [batch * num_chunks, hidden_size]\n",
        "        chunk_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Reshape back: [batch, num_chunks, hidden_size]\n",
        "        chunk_embeddings = chunk_embeddings.view(batch_size, num_chunks, -1)\n",
        "\n",
        "        return chunk_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        chunk_input_ids: torch.Tensor,\n",
        "        chunk_attention_masks: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            chunk_input_ids: [batch, num_chunks, max_length]\n",
        "            chunk_attention_masks: [batch, num_chunks, max_length]\n",
        "\n",
        "        Returns:\n",
        "            dict with:\n",
        "                - logits: [batch, num_classes]\n",
        "                - chunk_attention: [batch, num_chunks]\n",
        "        \"\"\"\n",
        "        # 1. Encode chunks: [batch, num_chunks, hidden_size]\n",
        "        chunk_embeddings = self.encode_chunks(chunk_input_ids, chunk_attention_masks)\n",
        "\n",
        "        # 2. Create chunk-level mask\n",
        "        chunk_mask = (chunk_attention_masks.sum(dim=2) > 0).float()\n",
        "\n",
        "        # 3. Apply chunk-level attention\n",
        "        doc_representation, chunk_attention_weights = self.chunk_attention(\n",
        "            chunk_embeddings,\n",
        "            mask=chunk_mask\n",
        "        )\n",
        "\n",
        "        # 4. Classification\n",
        "        logits = self.classifier(doc_representation)\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'chunk_attention': chunk_attention_weights\n",
        "        }\n",
        "\n",
        "print(\"✓ HierarchicalAttentionClassifier defined\")\n"
      ],
      "metadata": {
        "id": "OMlBnzozDmw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 10: CREATE DATASETS (UPDATED - THÊM TEST SET)\n",
        "# ================================================================================\n",
        "\n",
        "print(\"Creating training dataset...\")\n",
        "train_dataset = HierarchicalAttentionDataset(\n",
        "    df=train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    normalizer=normalizer,\n",
        "    retriever=retriever,\n",
        "    chunk_size=400,\n",
        "    top_k=5,\n",
        "    max_length=256,\n",
        "    min_chunks=3,\n",
        "    min_similarity=0.15,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"✓ Train dataset: {len(train_dataset)} samples\")\n",
        "\n",
        "print(\"\\nCreating validation dataset...\")\n",
        "val_dataset = HierarchicalAttentionDataset(\n",
        "    df=val_df,\n",
        "    tokenizer=tokenizer,\n",
        "    normalizer=normalizer,\n",
        "    retriever=retriever,\n",
        "    chunk_size=400,\n",
        "    top_k=5,\n",
        "    max_length=256,\n",
        "    min_chunks=3,\n",
        "    min_similarity=0.15,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"✓ Val dataset: {len(val_dataset)} samples\")\n",
        "\n",
        "print(\"\\nCreating test dataset...\")\n",
        "test_dataset = HierarchicalAttentionDataset(\n",
        "    df=test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    normalizer=normalizer,\n",
        "    retriever=retriever,\n",
        "    chunk_size=400,\n",
        "    top_k=5,\n",
        "    max_length=256,\n",
        "    min_chunks=3,\n",
        "    min_similarity=0.15,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"✓ Test dataset: {len(test_dataset)} samples\")\n",
        "\n",
        "# Test one sample\n",
        "print(\"\\n### Testing Dataset ###\")\n",
        "sample = train_dataset[0]\n",
        "print(f\"Sample shape:\")\n",
        "print(f\"  chunk_input_ids: {sample['chunk_input_ids'].shape}\")\n",
        "print(f\"  chunk_attention_masks: {sample['chunk_attention_masks'].shape}\")\n",
        "print(f\"  label: {sample['label']}\")\n",
        "\n",
        "# Check for empty chunks\n",
        "for i in range(5):\n",
        "    num_tokens = (sample['chunk_input_ids'][i] != tokenizer.pad_token_id).sum().item()\n",
        "    print(f\"  Chunk {i}: {num_tokens} tokens\")\n",
        "\n",
        "min_tokens = min(\n",
        "    (sample['chunk_input_ids'][i] != tokenizer.pad_token_id).sum().item()\n",
        "    for i in range(5)\n",
        ")\n",
        "print(f\"\\n{'✅ GOOD' if min_tokens > 10 else '❌ ISSUE'}: Min tokens = {min_tokens}\")\n"
      ],
      "metadata": {
        "id": "IxJfzshXDoYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 11: TRAINING SETUP (FIXED - NO MULTIPROCESSING WITH CUDA)\n",
        "# ================================================================================\n",
        "\n",
        "# Initialize model\n",
        "model = HierarchicalAttentionClassifier(\n",
        "    phobert_name='vinai/phobert-base-v2',\n",
        "    chunk_attention_hidden=128,\n",
        "    num_classes=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"✓ Model initialized on {device}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# ========================================\n",
        "# FIX: SET num_workers=0 WHEN USING CUDA\n",
        "# ========================================\n",
        "use_cuda = torch.cuda.is_available()\n",
        "num_workers = 0  # MUST be 0 with CUDA + sentence_transformers\n",
        "\n",
        "if use_cuda:\n",
        "    print(\"\\n⚠️ CUDA detected: Setting num_workers=0 to avoid multiprocessing errors\")\n",
        "else:\n",
        "    num_workers = 2\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,  # ← FIXED\n",
        "    pin_memory=False  # ← FIXED: Also set to False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,  # ← FIXED\n",
        "    pin_memory=False  # ← FIXED\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,  # ← FIXED\n",
        "    pin_memory=False  # ← FIXED\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ DataLoaders created\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "print(f\"  num_workers: {num_workers} (CUDA multiprocessing disabled)\")\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Training components ready\")\n",
        "print(f\"  Optimizer: AdamW (lr=2e-5)\")\n",
        "print(f\"  Loss: CrossEntropyLoss\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau (patience=2)\")"
      ],
      "metadata": {
        "id": "5qkBZ3k7DroU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 12: TRAINING LOOP WITH EARLY STOPPING (SAVE TO DRIVE)\n",
        "# ================================================================================\n",
        "\n",
        "# ========================================\n",
        "# SETUP SAVE PATHS TO GOOGLE DRIVE\n",
        "# ========================================\n",
        "\n",
        "# Tạo thư mục trong Drive nếu chưa có\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v3'\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "# Define save paths\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "last_model_path = os.path.join(drive_save_dir, 'han_rag_last.pth')\n",
        "history_path = os.path.join(drive_save_dir, 'training_history.json')\n",
        "\n",
        "print(f\"✓ Models will be saved to: {drive_save_dir}\")\n",
        "print(f\"  - Best model: {best_model_path}\")\n",
        "print(f\"  - Last model: {last_model_path}\")\n",
        "print(f\"  - History: {history_path}\")\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_f1 = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'val_f1': [],\n",
        "    'val_precision': [],\n",
        "    'val_recall': []\n",
        "}\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EPOCH {epoch+1}/{num_epochs}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # ========================================\n",
        "    # TRAINING PHASE\n",
        "    # ========================================\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        chunk_input_ids = batch['chunk_input_ids'].to(device)\n",
        "        chunk_attention_masks = batch['chunk_attention_masks'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(chunk_input_ids, chunk_attention_masks)\n",
        "        logits = outputs['logits']\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{train_correct/train_total:.4f}'\n",
        "        })\n",
        "\n",
        "        # Log attention warnings\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            chunk_attention = outputs['chunk_attention']\n",
        "            avg_pad_ratio = (chunk_attention_masks == 0).float().mean().item()\n",
        "            if avg_pad_ratio > 0.7:\n",
        "                print(f\"\\n⚠️ High padding ratio: {avg_pad_ratio:.2%}\")\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # ========================================\n",
        "    # VALIDATION PHASE\n",
        "    # ========================================\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            chunk_input_ids = batch['chunk_input_ids'].to(device)\n",
        "            chunk_attention_masks = batch['chunk_attention_masks'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(chunk_input_ids, chunk_attention_masks)\n",
        "            logits = outputs['logits']\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    val_acc = accuracy_score(val_labels_list, val_preds)\n",
        "    val_f1 = f1_score(val_labels_list, val_preds, average='macro')\n",
        "    val_precision = precision_score(val_labels_list, val_preds, average='macro')\n",
        "    val_recall = recall_score(val_labels_list, val_preds, average='macro')\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    history['val_precision'].append(val_precision)\n",
        "    history['val_recall'].append(val_recall)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n### EPOCH {epoch+1} RESULTS ###\")\n",
        "    print(f\"Train Loss:     {train_loss:.4f}\")\n",
        "    print(f\"Train Acc:      {train_acc:.4f}\")\n",
        "    print(f\"Val Acc:        {val_acc:.4f}\")\n",
        "    print(f\"Val F1:         {val_f1:.4f}\")\n",
        "    print(f\"Val Precision:  {val_precision:.4f}\")\n",
        "    print(f\"Val Recall:     {val_recall:.4f}\")\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    # ========================================\n",
        "    # SAVE CHECKPOINT TO GOOGLE DRIVE\n",
        "    # ========================================\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'val_f1': val_f1,\n",
        "        'val_acc': val_acc,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'history': history,\n",
        "        'best_val_f1': best_val_f1\n",
        "    }\n",
        "\n",
        "    # Save last checkpoint (overwrite every epoch)\n",
        "    torch.save(checkpoint, last_model_path)\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_f1 > best_val_f1:\n",
        "        improvement = val_f1 - best_val_f1\n",
        "        best_val_f1 = val_f1\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Save best model to Drive\n",
        "        torch.save(checkpoint, best_model_path)\n",
        "        print(f\"✅ New best model saved! Val F1: {best_val_f1:.4f} (↑{improvement:.4f})\")\n",
        "        print(f\"   Saved to: {best_model_path}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\n🛑 Early stopping triggered!\")\n",
        "            print(f\"   Best Val F1: {best_val_f1:.4f} (Epoch {checkpoint['epoch'] - patience_counter})\")\n",
        "            break\n",
        "\n",
        "    # Save training history to JSON after each epoch\n",
        "    import json\n",
        "    with open(history_path, 'w') as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Val F1: {best_val_f1:.4f}\")\n",
        "print(f\"\\nSaved files in Google Drive:\")\n",
        "print(f\"  📁 {drive_save_dir}\")\n",
        "print(f\"    ├── han_rag_best.pth (best model)\")\n",
        "print(f\"    ├── han_rag_last.pth (last epoch)\")\n",
        "print(f\"    └── training_history.json\")"
      ],
      "metadata": {
        "id": "JETbfnInDt33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 13: PLOT TRAINING HISTORY (LOAD FROM DRIVE)\n",
        "# ================================================================================\n",
        "\n",
        "# Load history from Drive\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v3'\n",
        "history_path = os.path.join(drive_save_dir, 'training_history.json')\n",
        "\n",
        "print(f\"Loading training history from: {history_path}\")\n",
        "with open(history_path, 'r') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "print(f\"✓ Loaded history with {len(history['train_loss'])} epochs\")\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Loss\n",
        "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "axes[0, 1].plot(history['train_acc'], label='Train Acc', marker='o', linewidth=2)\n",
        "axes[0, 1].plot(history['val_acc'], label='Val Acc', marker='s', linewidth=2)\n",
        "axes[0, 1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: F1 Score\n",
        "axes[1, 0].plot(history['val_f1'], label='Val F1', marker='o', color='green', linewidth=2)\n",
        "axes[1, 0].set_title('Validation F1 Score', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('F1 Score')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Highlight best epoch\n",
        "best_epoch = history['val_f1'].index(max(history['val_f1']))\n",
        "axes[1, 0].scatter(best_epoch, max(history['val_f1']),\n",
        "                   color='red', s=100, zorder=5, label='Best')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Plot 4: Precision & Recall\n",
        "axes[1, 1].plot(history['val_precision'], label='Precision', marker='o', linewidth=2)\n",
        "axes[1, 1].plot(history['val_recall'], label='Recall', marker='s', linewidth=2)\n",
        "axes[1, 1].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Score')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save to Drive\n",
        "plot_path = os.path.join(drive_save_dir, 'training_history.png')\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Plot saved to: {plot_path}\")\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total epochs: {len(history['train_loss'])}\")\n",
        "print(f\"Best epoch: {best_epoch + 1}\")\n",
        "print(f\"Best Val F1: {max(history['val_f1']):.4f}\")\n",
        "print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
        "print(f\"Final Train Acc: {history['train_acc'][-1]:.4f}\")\n",
        "print(f\"Final Val F1: {history['val_f1'][-1]:.4f}\")"
      ],
      "metadata": {
        "id": "tKKr_RKKDvc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 14: VALIDATION SET SUMMARY (LOAD FROM DRIVE)\n",
        "# ================================================================================\n",
        "\n",
        "# Load best model from Drive\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v3'\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "\n",
        "print(f\"Loading best model from: {best_model_path}\")\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']} (Val F1={checkpoint['val_f1']:.4f})\")\n",
        "\n",
        "# Quick validation metrics\n",
        "model.eval()\n",
        "val_preds = []\n",
        "val_labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "        chunk_input_ids = batch['chunk_input_ids'].to(device)\n",
        "        chunk_attention_masks = batch['chunk_attention_masks'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(chunk_input_ids, chunk_attention_masks)\n",
        "        logits = outputs['logits']\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "        val_labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "val_acc = accuracy_score(val_labels_list, val_preds)\n",
        "val_f1 = f1_score(val_labels_list, val_preds, average='macro')\n",
        "val_precision = precision_score(val_labels_list, val_preds, average='macro')\n",
        "val_recall = recall_score(val_labels_list, val_preds, average='macro')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDATION SET METRICS (BEST MODEL)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:   {val_acc:.4f}\")\n",
        "print(f\"F1 Score:   {val_f1:.4f}\")\n",
        "print(f\"Precision:  {val_precision:.4f}\")\n",
        "print(f\"Recall:     {val_recall:.4f}\")\n",
        "\n",
        "# Simple confusion matrix\n",
        "cm = confusion_matrix(val_labels_list, val_preds)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  [[{cm[0,0]:4d}, {cm[0,1]:4d}]   (REAL)\")\n",
        "print(f\"   [{cm[1,0]:4d}, {cm[1,1]:4d}]]  (FAKE)\")\n",
        "print(f\"     REAL  FAKE\")\n",
        "\n",
        "print(\"\\n✓ Validation complete. Proceed to final test evaluation.\")"
      ],
      "metadata": {
        "id": "tZL__Y8TD1Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 15: TEST WITH EXAMPLE\n",
        "# ================================================================================\n",
        "\n",
        "def predict_news(title, content, model, tokenizer, normalizer, retriever, device):\n",
        "    \"\"\"Test model với 1 sample\"\"\"\n",
        "    # Create mini dataset\n",
        "    test_df = pd.DataFrame({\n",
        "        'title': [title],\n",
        "        'content': [content],\n",
        "        'label': [0]  # Dummy label\n",
        "    })\n",
        "\n",
        "    test_dataset = HierarchicalAttentionDataset(\n",
        "        df=test_df,\n",
        "        tokenizer=tokenizer,\n",
        "        normalizer=normalizer,\n",
        "        retriever=retriever,\n",
        "        chunk_size=400,\n",
        "        top_k=5,\n",
        "        max_length=256,\n",
        "        min_chunks=3\n",
        "    )\n",
        "\n",
        "    sample = test_dataset[0]\n",
        "    chunk_input_ids = sample['chunk_input_ids'].unsqueeze(0).to(device)\n",
        "    chunk_attention_masks = sample['chunk_attention_masks'].unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(chunk_input_ids, chunk_attention_masks)\n",
        "        logits = outputs['logits']\n",
        "        chunk_attention = outputs['chunk_attention']\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_class = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PREDICTION RESULT\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nTitle: {title}\")\n",
        "    print(f\"Content (first 200 chars): {content[:200]}...\")\n",
        "    print(f\"\\nPrediction: {'FAKE' if pred_class == 1 else 'REAL'}\")\n",
        "    print(f\"Confidence: {probs[0, pred_class].item():.4f}\")\n",
        "    print(f\"Probabilities: REAL={probs[0,0]:.4f}, FAKE={probs[0,1]:.4f}\")\n",
        "\n",
        "    print(f\"\\n### Chunk Attention Weights ###\")\n",
        "    for i in range(5):\n",
        "        num_tokens = (chunk_input_ids[0, i] != tokenizer.pad_token_id).sum().item()\n",
        "        attention = chunk_attention[0, i].item()\n",
        "        print(f\"Chunk {i}: {attention:.4f} ({num_tokens} tokens)\")\n",
        "\n",
        "# Test with COVID-19 example\n",
        "test_title = \"NÓNG: Thế giới đối mặt với COVID-19\"\n",
        "test_content = \"\"\"NÓNG ! 15/3 sẽ là ngày đáng nhớ cho không chỉ Việt Nam mà của toàn thế giới do những gì COVID-19 gây ra !!! - Rạng sáng 15/3, Pháp ra lệnh đóng cửa toàn bộ nhà hàng, rạp chiếu phim, cửa hàng,..trừ.. siêu thị, trạm xăng, ngân hàng, tabac, presse báo chí và pharmacie. Phong tỏa 1 phần đất nước. - Chủ tịch hiệp hội Y Tế Ý vừa qua đời vì Corona Virus. Hưởng thọ 67 tuổi.. - Vợ thủ tướng Canada đã dương tính Corona Virus . Ông cũng đang bị cách ly\"\"\"\n",
        "\n",
        "predict_news(test_title, test_content, model, tokenizer, normalizer, retriever, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ ALL TESTS COMPLETED\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "shJtNSN8D3_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 16: FINAL EVALUATION ON TEST SET (NEW)\n",
        "# ================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL EVALUATION ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load best model\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v3'\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "\n",
        "print(f\"Loading best model from: {best_model_path}\")\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']} (Val F1={checkpoint['val_f1']:.4f})\")\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels_list = []\n",
        "test_probs = []\n",
        "test_attention_weights = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        chunk_input_ids = batch['chunk_input_ids'].to(device)\n",
        "        chunk_attention_masks = batch['chunk_attention_masks'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(chunk_input_ids, chunk_attention_masks)\n",
        "        logits = outputs['logits']\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels_list.extend(labels.cpu().numpy())\n",
        "        test_probs.extend(probs.cpu().numpy())\n",
        "        test_attention_weights.extend(outputs['chunk_attention'].cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "test_acc = accuracy_score(test_labels_list, test_preds)\n",
        "test_f1 = f1_score(test_labels_list, test_preds, average='macro')\n",
        "test_f1_weighted = f1_score(test_labels_list, test_preds, average='weighted')\n",
        "test_precision = precision_score(test_labels_list, test_preds, average='macro')\n",
        "test_recall = recall_score(test_labels_list, test_preds, average='macro')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:          {test_acc:.4f}\")\n",
        "print(f\"F1 Score (Macro):  {test_f1:.4f}\")\n",
        "print(f\"F1 Score (Weighted): {test_f1_weighted:.4f}\")\n",
        "print(f\"Precision:         {test_precision:.4f}\")\n",
        "print(f\"Recall:            {test_recall:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSIFICATION REPORT (TEST SET)\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(\n",
        "    test_labels_list,\n",
        "    test_preds,\n",
        "    target_names=['REAL', 'FAKE'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_labels_list, test_preds)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nTrue Negatives (REAL predicted as REAL):  {cm[0,0]}\")\n",
        "print(f\"False Positives (REAL predicted as FAKE): {cm[0,1]}\")\n",
        "print(f\"False Negatives (FAKE predicted as REAL): {cm[1,0]}\")\n",
        "print(f\"True Positives (FAKE predicted as FAKE):  {cm[1,1]}\")\n",
        "\n",
        "# Analyze attention patterns\n",
        "test_attention_weights = np.array(test_attention_weights)\n",
        "mean_attention = test_attention_weights.mean(axis=0)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ATTENTION ANALYSIS (TEST SET)\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nAverage attention weights across all test samples:\")\n",
        "for i, weight in enumerate(mean_attention):\n",
        "    bar_length = int(weight * 40 / mean_attention.max())\n",
        "    bar = \"█\" * bar_length\n",
        "    print(f\"Chunk {i}: {bar} {weight:.4f}\")\n",
        "\n",
        "# Compare train/val/test performance\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"{'Split':<15} {'Accuracy':<12} {'F1 (Macro)':<12}\")\n",
        "print(f\"{'-'*40}\")\n",
        "print(f\"{'Validation':<15} {val_acc:.4f}       {checkpoint['val_f1']:.4f}\")\n",
        "print(f\"{'Test':<15} {test_acc:.4f}       {test_f1:.4f}\")\n",
        "\n",
        "gap = abs(checkpoint['val_f1'] - test_f1)\n",
        "if gap < 0.02:\n",
        "    print(f\"\\n✅ GOOD: Val-Test gap = {gap:.4f} (< 0.02) → Model generalizes well\")\n",
        "elif gap < 0.05:\n",
        "    print(f\"\\n⚠️ ACCEPTABLE: Val-Test gap = {gap:.4f} (< 0.05) → Minor overfitting\")\n",
        "else:\n",
        "    print(f\"\\n❌ WARNING: Val-Test gap = {gap:.4f} (> 0.05) → Significant overfitting!\")\n",
        "\n",
        "# Save test results\n",
        "test_results = {\n",
        "    'test_accuracy': test_acc,\n",
        "    'test_f1_macro': test_f1,\n",
        "    'test_f1_weighted': test_f1_weighted,\n",
        "    'test_precision': test_precision,\n",
        "    'test_recall': test_recall,\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'mean_attention_weights': mean_attention.tolist()\n",
        "}\n",
        "\n",
        "# Save test results to Drive\n",
        "test_results_path = os.path.join(drive_save_dir, 'test_results.json')\n",
        "with open(test_results_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n✓ Detailed test results saved to: {test_results_path}\")"
      ],
      "metadata": {
        "id": "gusUftXuEC3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 17: VISUALIZE TEST RESULTS (NEW)\n",
        "# ================================================================================\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot 1: Confusion Matrix\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=['REAL', 'FAKE'],\n",
        "    yticklabels=['REAL', 'FAKE'],\n",
        "    ax=axes[0],\n",
        "    cbar_kws={'label': 'Count'}\n",
        ")\n",
        "axes[0].set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "\n",
        "# Plot 2: Attention Distribution\n",
        "chunk_labels = [f'Chunk {i}' for i in range(5)]\n",
        "axes[1].bar(chunk_labels, mean_attention, color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "axes[1].set_title('Average Attention Weights (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Attention Weight')\n",
        "axes[1].set_xlabel('Chunk Position')\n",
        "axes[1].set_ylim([0, mean_attention.max() * 1.2])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add values on bars\n",
        "for i, v in enumerate(mean_attention):\n",
        "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_results_visualization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Test results visualized and saved\")\n"
      ],
      "metadata": {
        "id": "z9XKvoygEL49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 18: EXPORT MODEL TO ONNX (NEW)\n",
        "# ================================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXPORTING MODEL TO ONNX FORMAT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Install ONNX packages if not available\n",
        "try:\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "except ImportError:\n",
        "    print(\"Installing ONNX dependencies...\")\n",
        "    !pip install -q onnx onnxruntime\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "\n",
        "# ========================================\n",
        "# 1. LOAD BEST MODEL\n",
        "# ========================================\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v3'\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "\n",
        "checkpoint = torch.load(best_model_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "# ========================================\n",
        "# 2. CREATE DUMMY INPUT\n",
        "# ========================================\n",
        "# ONNX cần dummy input để trace model\n",
        "batch_size = 1\n",
        "num_chunks = 5\n",
        "max_length = 256\n",
        "\n",
        "dummy_chunk_input_ids = torch.randint(\n",
        "    0, tokenizer.vocab_size,\n",
        "    (batch_size, num_chunks, max_length),\n",
        "    dtype=torch.long\n",
        ").to(device)\n",
        "\n",
        "dummy_chunk_attention_masks = torch.ones(\n",
        "    (batch_size, num_chunks, max_length),\n",
        "    dtype=torch.long\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\n✓ Created dummy inputs:\")\n",
        "print(f\"  chunk_input_ids shape: {dummy_chunk_input_ids.shape}\")\n",
        "print(f\"  chunk_attention_masks shape: {dummy_chunk_attention_masks.shape}\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    test_output = model(dummy_chunk_input_ids, dummy_chunk_attention_masks)\n",
        "    print(f\"\\n✓ Test forward pass successful:\")\n",
        "    print(f\"  logits shape: {test_output['logits'].shape}\")\n",
        "    print(f\"  chunk_attention shape: {test_output['chunk_attention'].shape}\")\n",
        "\n",
        "# ========================================\n",
        "# 3. EXPORT TO ONNX\n",
        "# ========================================\n",
        "onnx_path = os.path.join(drive_save_dir, 'han_rag_model.onnx')\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"EXPORTING TO ONNX...\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Input names\n",
        "input_names = ['chunk_input_ids', 'chunk_attention_masks']\n",
        "output_names = ['logits', 'chunk_attention']\n",
        "\n",
        "# Dynamic axes (để support batch size linh hoạt)\n",
        "dynamic_axes = {\n",
        "    'chunk_input_ids': {0: 'batch_size'},\n",
        "    'chunk_attention_masks': {0: 'batch_size'},\n",
        "    'logits': {0: 'batch_size'},\n",
        "    'chunk_attention': {0: 'batch_size'}\n",
        "}\n",
        "\n",
        "# Export\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (dummy_chunk_input_ids, dummy_chunk_attention_masks),\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=14,  # ONNX opset version\n",
        "    do_constant_folding=True,\n",
        "    input_names=input_names,\n",
        "    output_names=output_names,\n",
        "    dynamic_axes=dynamic_axes,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"✅ Model exported to: {onnx_path}\")\n",
        "\n",
        "# Check file size\n",
        "onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "print(f\"   File size: {onnx_size_mb:.2f} MB\")\n",
        "\n",
        "# ========================================\n",
        "# 4. VALIDATE ONNX MODEL\n",
        "# ========================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"VALIDATING ONNX MODEL\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Load and check ONNX model\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"✓ ONNX model is valid\")\n",
        "\n",
        "# Print model info\n",
        "print(f\"\\nONNX Model Info:\")\n",
        "print(f\"  IR Version: {onnx_model.ir_version}\")\n",
        "print(f\"  Producer: {onnx_model.producer_name}\")\n",
        "print(f\"  Opset Version: {onnx_model.opset_import[0].version}\")\n",
        "\n",
        "print(f\"\\nInputs:\")\n",
        "for input_tensor in onnx_model.graph.input:\n",
        "    print(f\"  - {input_tensor.name}: {[d.dim_value if d.dim_value > 0 else 'dynamic' for d in input_tensor.type.tensor_type.shape.dim]}\")\n",
        "\n",
        "print(f\"\\nOutputs:\")\n",
        "for output_tensor in onnx_model.graph.output:\n",
        "    print(f\"  - {output_tensor.name}: {[d.dim_value if d.dim_value > 0 else 'dynamic' for d in output_tensor.type.tensor_type.shape.dim]}\")\n",
        "\n",
        "# ========================================\n",
        "# 5. TEST ONNX INFERENCE\n",
        "# ========================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TESTING ONNX INFERENCE\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Create ONNX Runtime session\n",
        "ort_session = ort.InferenceSession(\n",
        "    onnx_path,\n",
        "    providers=['CPUExecutionProvider']  # Use CPU for compatibility\n",
        ")\n",
        "\n",
        "print(f\"✓ ONNX Runtime session created\")\n",
        "print(f\"  Providers: {ort_session.get_providers()}\")\n",
        "\n",
        "# Prepare test input (use a real sample from test set)\n",
        "test_sample = test_dataset[0]\n",
        "test_input_ids = test_sample['chunk_input_ids'].unsqueeze(0).numpy()\n",
        "test_attention_masks = test_sample['chunk_attention_masks'].unsqueeze(0).numpy()\n",
        "\n",
        "# Run ONNX inference\n",
        "onnx_inputs = {\n",
        "    'chunk_input_ids': test_input_ids,\n",
        "    'chunk_attention_masks': test_attention_masks\n",
        "}\n",
        "\n",
        "onnx_outputs = ort_session.run(None, onnx_inputs)\n",
        "onnx_logits = onnx_outputs[0]\n",
        "onnx_attention = onnx_outputs[1]\n",
        "\n",
        "print(f\"\\n✓ ONNX inference successful:\")\n",
        "print(f\"  Logits shape: {onnx_logits.shape}\")\n",
        "print(f\"  Attention shape: {onnx_attention.shape}\")\n",
        "\n",
        "# Compare with PyTorch output\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pytorch_outputs = model(\n",
        "        test_sample['chunk_input_ids'].unsqueeze(0).to(device),\n",
        "        test_sample['chunk_attention_masks'].unsqueeze(0).to(device)\n",
        "    )\n",
        "    pytorch_logits = pytorch_outputs['logits'].cpu().numpy()\n",
        "    pytorch_attention = pytorch_outputs['chunk_attention'].cpu().numpy()\n",
        "\n",
        "# Calculate difference\n",
        "logits_diff = np.abs(onnx_logits - pytorch_logits).max()\n",
        "attention_diff = np.abs(onnx_attention - pytorch_attention).max()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"PYTORCH vs ONNX COMPARISON\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Max logits difference: {logits_diff:.6f}\")\n",
        "print(f\"Max attention difference: {attention_diff:.6f}\")\n",
        "\n",
        "if logits_diff < 1e-4 and attention_diff < 1e-4:\n",
        "    print(\"✅ EXCELLENT: Outputs match within tolerance (< 1e-4)\")\n",
        "elif logits_diff < 1e-3 and attention_diff < 1e-3:\n",
        "    print(\"✅ GOOD: Outputs match within acceptable tolerance (< 1e-3)\")\n",
        "else:\n",
        "    print(\"⚠️ WARNING: Significant difference detected!\")\n",
        "\n",
        "# Show predictions\n",
        "pytorch_pred = np.argmax(pytorch_logits, axis=1)[0]\n",
        "onnx_pred = np.argmax(onnx_logits, axis=1)[0]\n",
        "\n",
        "print(f\"\\nPredictions:\")\n",
        "print(f\"  PyTorch: {pytorch_pred} ({'FAKE' if pytorch_pred == 1 else 'REAL'})\")\n",
        "print(f\"  ONNX:    {onnx_pred} ({'FAKE' if onnx_pred == 1 else 'REAL'})\")\n",
        "print(f\"  Match: {'✅ YES' if pytorch_pred == onnx_pred else '❌ NO'}\")\n",
        "\n",
        "# ========================================\n",
        "# 6. BENCHMARK INFERENCE SPEED\n",
        "# ========================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"BENCHMARKING INFERENCE SPEED\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "import time\n",
        "\n",
        "num_runs = 100\n",
        "\n",
        "# PyTorch benchmark\n",
        "model.to('cpu')  # Fair comparison on CPU\n",
        "pytorch_times = []\n",
        "for _ in range(num_runs):\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(\n",
        "            test_sample['chunk_input_ids'].unsqueeze(0),\n",
        "            test_sample['chunk_attention_masks'].unsqueeze(0)\n",
        "        )\n",
        "    pytorch_times.append(time.time() - start)\n",
        "\n",
        "pytorch_avg = np.mean(pytorch_times) * 1000  # Convert to ms\n",
        "\n",
        "# ONNX benchmark\n",
        "onnx_times = []\n",
        "for _ in range(num_runs):\n",
        "    start = time.time()\n",
        "    _ = ort_session.run(None, onnx_inputs)\n",
        "    onnx_times.append(time.time() - start)\n",
        "\n",
        "onnx_avg = np.mean(onnx_times) * 1000  # Convert to ms\n",
        "\n",
        "print(f\"\\nInference time (average over {num_runs} runs):\")\n",
        "print(f\"  PyTorch: {pytorch_avg:.2f} ms\")\n",
        "print(f\"  ONNX:    {onnx_avg:.2f} ms\")\n",
        "print(f\"  Speedup: {pytorch_avg/onnx_avg:.2f}x\")\n",
        "\n",
        "if onnx_avg < pytorch_avg:\n",
        "    print(f\"✅ ONNX is {(pytorch_avg/onnx_avg - 1)*100:.1f}% faster!\")\n",
        "else:\n",
        "    print(f\"⚠️ PyTorch is {(onnx_avg/pytorch_avg - 1)*100:.1f}% faster\")\n",
        "\n",
        "# ========================================\n",
        "# 7. SAVE METADATA\n",
        "# ========================================\n",
        "onnx_metadata = {\n",
        "    'model_type': 'HierarchicalAttentionClassifier',\n",
        "    'framework': 'PyTorch -> ONNX',\n",
        "    'opset_version': 14,\n",
        "    'input_shapes': {\n",
        "        'chunk_input_ids': [batch_size, num_chunks, max_length],\n",
        "        'chunk_attention_masks': [batch_size, num_chunks, max_length]\n",
        "    },\n",
        "    'output_shapes': {\n",
        "        'logits': [batch_size, 2],\n",
        "        'chunk_attention': [batch_size, num_chunks]\n",
        "    },\n",
        "    'model_config': {\n",
        "        'phobert_name': 'vinai/phobert-base-v2',\n",
        "        'chunk_attention_hidden': 128,\n",
        "        'num_classes': 2,\n",
        "        'dropout': 0.3,\n",
        "        'chunk_size': 400,\n",
        "        'top_k': 5,\n",
        "        'max_length': 256\n",
        "    },\n",
        "    'training_info': {\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'val_f1': float(checkpoint['val_f1']),\n",
        "        'val_acc': float(checkpoint.get('val_acc', 0))\n",
        "    },\n",
        "    'validation': {\n",
        "        'max_logits_diff': float(logits_diff),\n",
        "        'max_attention_diff': float(attention_diff),\n",
        "        'predictions_match': bool(pytorch_pred == onnx_pred)\n",
        "    },\n",
        "    'performance': {\n",
        "        'pytorch_inference_ms': float(pytorch_avg),\n",
        "        'onnx_inference_ms': float(onnx_avg),\n",
        "        'speedup': float(pytorch_avg/onnx_avg)\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(drive_save_dir, 'onnx_metadata.json')\n",
        "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(onnx_metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n✓ Metadata saved to: {metadata_path}\")\n",
        "\n",
        "# ========================================\n",
        "# 8. SUMMARY\n",
        "# ========================================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ONNX EXPORT SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\n📁 Exported files in: {drive_save_dir}\")\n",
        "print(f\"   ├── han_rag_model.onnx ({onnx_size_mb:.2f} MB)\")\n",
        "print(f\"   └── onnx_metadata.json\")\n",
        "print(f\"\\n✅ Model successfully exported and validated!\")\n",
        "print(f\"   - Outputs match PyTorch (diff < {max(logits_diff, attention_diff):.6f})\")\n",
        "print(f\"   - ONNX inference: {onnx_avg:.2f} ms\")\n",
        "print(f\"   - Ready for deployment!\")\n"
      ],
      "metadata": {
        "id": "3Vuyw8iBHKSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 19: TEST ONNX MODEL WITH TEXT INPUT (NEW)\n",
        "# ================================================================================\n",
        "\n",
        "def predict_with_onnx(\n",
        "    title: str,\n",
        "    content: str,\n",
        "    ort_session: ort.InferenceSession,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    retriever,\n",
        "    device='cpu'\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Predict using ONNX model with raw text input\n",
        "    \"\"\"\n",
        "    # Create mini dataset\n",
        "    test_df = pd.DataFrame({\n",
        "        'title': [title],\n",
        "        'content': [content],\n",
        "        'label': [0]  # Dummy\n",
        "    })\n",
        "\n",
        "    test_dataset = HierarchicalAttentionDataset(\n",
        "        df=test_df,\n",
        "        tokenizer=tokenizer,\n",
        "        normalizer=normalizer,\n",
        "        retriever=retriever,\n",
        "        chunk_size=400,\n",
        "        top_k=5,\n",
        "        max_length=256,\n",
        "        min_chunks=3\n",
        "    )\n",
        "\n",
        "    sample = test_dataset[0]\n",
        "\n",
        "    # Prepare ONNX inputs\n",
        "    onnx_inputs = {\n",
        "        'chunk_input_ids': sample['chunk_input_ids'].unsqueeze(0).numpy(),\n",
        "        'chunk_attention_masks': sample['chunk_attention_masks'].unsqueeze(0).numpy()\n",
        "    }\n",
        "\n",
        "    # Run inference\n",
        "    onnx_outputs = ort_session.run(None, onnx_inputs)\n",
        "    logits = onnx_outputs[0][0]  # [2]\n",
        "    chunk_attention = onnx_outputs[1][0]  # [5]\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "    pred_class = np.argmax(probs)\n",
        "\n",
        "    return {\n",
        "        'prediction': 'FAKE' if pred_class == 1 else 'REAL',\n",
        "        'confidence': float(probs[pred_class]),\n",
        "        'probabilities': {\n",
        "            'REAL': float(probs[0]),\n",
        "            'FAKE': float(probs[1])\n",
        "        },\n",
        "        'chunk_attention': chunk_attention.tolist()\n",
        "    }\n",
        "\n",
        "\n",
        "# Test với COVID-19 example\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING ONNX MODEL WITH TEXT INPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_title = \"NÓNG: Thế giới đối mặt với COVID-19\"\n",
        "test_content = \"\"\"NÓNG ! 15/3 sẽ là ngày đáng nhớ cho không chỉ Việt Nam mà của toàn thế giới do những gì COVID-19 gây ra !!! - Rạng sáng 15/3, Pháp ra lệnh đóng cửa toàn bộ nhà hàng, rạp chiếu phim, cửa hàng,..trừ.. siêu thị, trạm xăng, ngân hàng, tabac, presse báo chí và pharmacie.\"\"\"\n",
        "\n",
        "print(f\"\\nInput:\")\n",
        "print(f\"  Title: {test_title}\")\n",
        "print(f\"  Content: {test_content[:150]}...\")\n",
        "\n",
        "# Predict\n",
        "result = predict_with_onnx(\n",
        "    test_title,\n",
        "    test_content,\n",
        "    ort_session,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    retriever\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ONNX PREDICTION RESULT\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Prediction: {result['prediction']}\")\n",
        "print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "print(f\"\\nProbabilities:\")\n",
        "print(f\"  REAL: {result['probabilities']['REAL']:.4f}\")\n",
        "print(f\"  FAKE: {result['probabilities']['FAKE']:.4f}\")\n",
        "\n",
        "print(f\"\\nChunk Attention Weights:\")\n",
        "for i, weight in enumerate(result['chunk_attention']):\n",
        "    bar_length = int(weight * 40 / max(result['chunk_attention']))\n",
        "    bar = \"█\" * bar_length\n",
        "    print(f\"  Chunk {i}: {bar} {weight:.4f}\")\n",
        "\n",
        "print(\"\\n✅ ONNX model inference completed successfully!\")\n"
      ],
      "metadata": {
        "id": "9qKaa5XAHXUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 21: SAVE DEPLOYMENT PACKAGE (THÊM SAU CELL 18)\n",
        "# ================================================================================\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CREATING DEPLOYMENT PACKAGE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v3'\n",
        "deployment_dir = os.path.join(drive_save_dir, 'deployment_package')\n",
        "os.makedirs(deployment_dir, exist_ok=True)\n",
        "\n",
        "# ========================================\n",
        "# 1. SAVE TOKENIZER\n",
        "# ========================================\n",
        "print(\"\\n1. Saving tokenizer...\")\n",
        "tokenizer_dir = os.path.join(deployment_dir, 'tokenizer')\n",
        "tokenizer.save_pretrained(tokenizer_dir)\n",
        "print(f\"✓ Tokenizer saved: {os.listdir(tokenizer_dir)}\")\n",
        "\n",
        "# ========================================\n",
        "# 2. SAVE RETRIEVER\n",
        "# ========================================\n",
        "print(\"\\n2. Saving retriever...\")\n",
        "retriever_dir = os.path.join(deployment_dir, 'retriever')\n",
        "retriever.save(retriever_dir)\n",
        "print(f\"✓ Retriever saved\")\n",
        "\n",
        "# ========================================\n",
        "# 3. COPY ONNX\n",
        "# ========================================\n",
        "print(\"\\n3. Copying ONNX model...\")\n",
        "shutil.copy(\n",
        "    os.path.join(drive_save_dir, 'han_rag_model.onnx'),\n",
        "    os.path.join(deployment_dir, 'han_rag_model.onnx')\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# 4. SAVE CONFIG\n",
        "# ========================================\n",
        "print(\"\\n4. Saving config...\")\n",
        "config = {\n",
        "    'model_version': 'v2',\n",
        "    'best_epoch': checkpoint['epoch'],\n",
        "    'val_f1': float(checkpoint['val_f1']),\n",
        "    'model_config': {\n",
        "        'chunk_size': 400,\n",
        "        'top_k': 5,\n",
        "        'max_length': 256\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(deployment_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# ========================================\n",
        "# 5. CREATE ZIP\n",
        "# ========================================\n",
        "print(\"\\n5. Creating ZIP archive...\")\n",
        "shutil.make_archive(\n",
        "    os.path.join(drive_save_dir, 'deployment_package'),\n",
        "    'zip',\n",
        "    deployment_dir\n",
        ")\n",
        "\n",
        "zip_path = os.path.join(drive_save_dir, 'deployment_package.zip')\n",
        "zip_size = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ DEPLOYMENT PACKAGE READY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n📦 ZIP: {zip_path} ({zip_size:.2f} MB)\")\n",
        "print(f\"\\nContents:\")\n",
        "print(f\"  ├── han_rag_model.onnx\")\n",
        "print(f\"  ├── tokenizer/ (PhoBERT)\")\n",
        "print(f\"  ├── retriever/ (Vietnamese SBERT)\")\n",
        "print(f\"  └── config.json\")\n"
      ],
      "metadata": {
        "id": "UvMwNlfugNth"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}