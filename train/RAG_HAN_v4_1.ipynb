{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Installation\n",
        "%pip install transformers==4.35.0\n",
        "%pip install torch torchvision torchaudio\n",
        "%pip install underthesea\n",
        "%pip install onnx onnxruntime\n",
        "%pip install optimum[onnxruntime]\n",
        "%pip install datasets\n",
        "%pip install accelerate -U\n",
        "%pip install onnxscript"
      ],
      "metadata": {
        "id": "NuO6qolHHiHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 1: IMPORTS VÀ SETUP\n",
        "# ================================================================================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "import torch\n",
        "import unicodedata\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Check dependencies\n",
        "try:\n",
        "    from underthesea import word_tokenize, sent_tokenize\n",
        "    UNDERTHESEA_AVAILABLE = True\n",
        "    print(\"✓ underthesea available\")\n",
        "except ImportError:\n",
        "    UNDERTHESEA_AVAILABLE = False\n",
        "    print(\"⚠️ underthesea not available\")\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✓ Device: {device}\")"
      ],
      "metadata": {
        "id": "Q0W4lbhsCmRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_PZyZ_-CkE9"
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# CELL 2: LOAD DATA\n",
        "# ================================================================================\n",
        "\n",
        "# Mount Google Drive (nếu dùng Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your dataset\n",
        "# Thay đổi path theo file của bạn\n",
        "data_path = '/content/drive/MyDrive/FakeNewsModels/dataset_balanced.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Kiểm tra data\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nSample data:\")\n",
        "print(df.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 3: TRAIN/VAL/TEST SPLIT (FIXED)\n",
        "# ================================================================================\n",
        "\n",
        "# Split data theo tỷ lệ: 70% train, 15% val, 15% test\n",
        "print(\"Splitting dataset into train/val/test...\")\n",
        "\n",
        "# Bước 1: Split train vs (val+test)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.30,  # 30% cho val+test\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# Bước 2: Split (val+test) thành val và test\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.50,  # 50% của 30% = 15% cho test\n",
        "    random_state=42,\n",
        "    stratify=temp_df['label']\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Dataset Split Summary\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total samples:  {len(df):,}\")\n",
        "print(f\"Train set:      {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Val set:        {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test set:       {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'Label':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
        "print(f\"{'-'*40}\")\n",
        "for label in sorted(df['label'].unique()):\n",
        "    train_count = (train_df['label'] == label).sum()\n",
        "    val_count = (val_df['label'] == label).sum()\n",
        "    test_count = (test_df['label'] == label).sum()\n",
        "    print(f\"{label:<10} {train_count:<10} {val_count:<10} {test_count:<10}\")\n",
        "\n",
        "# Reset index\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(\"\\n✓ Data split completed\")\n"
      ],
      "metadata": {
        "id": "SDEuZ4KPDNpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 4: TEXT NORMALIZER (OPTIMIZED)\n",
        "# ================================================================================\n",
        "\n",
        "class VietnameseTextNormalizer:\n",
        "    \"\"\"Normalizer cho PhoBERT với caching và error handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.use_word_segment = UNDERTHESEA_AVAILABLE\n",
        "\n",
        "        # Cache word_tokenize function để tránh import lại\n",
        "        self._word_tokenize = None\n",
        "        if self.use_word_segment:\n",
        "            try:\n",
        "                from underthesea import word_tokenize\n",
        "                self._word_tokenize = word_tokenize\n",
        "            except ImportError:\n",
        "                self.use_word_segment = False\n",
        "\n",
        "        # Compile regex một lần (tối ưu performance)\n",
        "        self.url_pattern = re.compile(r'http[s]?://\\S+')\n",
        "        self.special_chars_pattern = re.compile(\n",
        "            r'[^\\w\\s.,!?àáảãạăằắẳẵặâầấẩẫậèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵđĐ]'\n",
        "        )\n",
        "        self.whitespace_pattern = re.compile(r'\\s+')\n",
        "\n",
        "    def normalize_unicode(self, text: str) -> str:\n",
        "        return unicodedata.normalize('NFC', text)\n",
        "\n",
        "    def clean_special_chars(self, text: str) -> str:\n",
        "        # Remove URLs\n",
        "        text = self.url_pattern.sub(' ', text)\n",
        "        # Remove special chars\n",
        "        text = self.special_chars_pattern.sub(' ', text)\n",
        "        return text\n",
        "\n",
        "    def word_segment(self, text: str) -> str:\n",
        "        if not self.use_word_segment or not self._word_tokenize:\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            return self._word_tokenize(text, format=\"text\")\n",
        "        except Exception as e:\n",
        "            # Fallback: trả về text gốc nếu lỗi\n",
        "            return text\n",
        "\n",
        "    def normalize(self, text: Optional[str], preserve_mask: bool = False) -> str:\n",
        "        if not text or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Unicode normalization\n",
        "        text = self.normalize_unicode(text)\n",
        "\n",
        "        # Clean special chars\n",
        "        text = text.strip()\n",
        "        text = self.clean_special_chars(text)\n",
        "        text = self.whitespace_pattern.sub(' ', text)\n",
        "\n",
        "        # Word segmentation\n",
        "        if not preserve_mask:\n",
        "            text = self.word_segment(text)\n",
        "            # Clean whitespace again (word_tokenize có thể tạo thêm spaces)\n",
        "            text = self.whitespace_pattern.sub(' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "# Initialize\n",
        "normalizer = VietnameseTextNormalizer()\n",
        "print(\"✓ Text normalizer initialized\")"
      ],
      "metadata": {
        "id": "tZGvgV0dDZO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 5: SEMANTIC CHUNKER (FIXED VERSION)\n",
        "# ================================================================================\n",
        "\n",
        "class SemanticChunkRetriever:\n",
        "    def __init__(self, chunk_size=400, chunk_overlap=50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def chunk_document(self, text):\n",
        "        # Dùng sent_tokenize\n",
        "        try:\n",
        "            sentences = sent_tokenize(text)\n",
        "        except:\n",
        "            sentences = re.split(r'[.!?]\\s+', text)\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_len = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent = sent.strip()\n",
        "            if not sent:\n",
        "                continue\n",
        "\n",
        "            sent_len = len(sent)\n",
        "\n",
        "            # Handle câu đơn quá dài TRƯỚC KHI thêm vào chunk\n",
        "            if sent_len > self.chunk_size * 1.5:\n",
        "                # Lưu chunk hiện tại nếu có\n",
        "                if current_chunk:\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                    current_chunk = []\n",
        "                    current_len = 0\n",
        "\n",
        "                # Split câu dài thành sub-chunks\n",
        "                words = sent.split()\n",
        "                for i in range(0, len(words), 50):\n",
        "                    sub_chunk = ' '.join(words[i:i+50])\n",
        "                    chunks.append(sub_chunk)\n",
        "                continue\n",
        "\n",
        "            # Nếu thêm câu này vượt quá chunk_size\n",
        "            if current_len + sent_len > self.chunk_size:\n",
        "                if current_chunk:\n",
        "                    # Lưu chunk hiện tại\n",
        "                    chunk_text = ' '.join(current_chunk)\n",
        "                    chunks.append(chunk_text)\n",
        "\n",
        "                    # Tạo overlap bằng cách giữ N câu cuối\n",
        "                    # Tính số câu cần giữ để overlap ~50 chars\n",
        "                    overlap_sents = []\n",
        "                    overlap_len = 0\n",
        "\n",
        "                    for s in reversed(current_chunk):\n",
        "                        if overlap_len + len(s) <= self.chunk_overlap:\n",
        "                            overlap_sents.insert(0, s)\n",
        "                            overlap_len += len(s) + 1\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    # Reset chunk với overlap\n",
        "                    current_chunk = overlap_sents\n",
        "                    current_len = overlap_len\n",
        "\n",
        "            # Thêm câu hiện tại\n",
        "            current_chunk.append(sent)\n",
        "            current_len += sent_len + 1\n",
        "\n",
        "        # Lưu chunk cuối\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# Initialize\n",
        "chunker = SemanticChunkRetriever(chunk_size=400, chunk_overlap=50)\n",
        "print(\"✓ Semantic chunker initialized with overlap\")"
      ],
      "metadata": {
        "id": "Mdfzyu-bDg6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 6: LOAD MODELS (TOKENIZER & RETRIEVER)\n",
        "# ================================================================================\n",
        "\n",
        "print(\"Loading PhoBERT tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')\n",
        "print(\"✓ PhoBERT tokenizer loaded\")\n",
        "\n",
        "print(\"\\nLoading Vietnamese-SBERT for RAG retrieval...\")\n",
        "retriever = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
        "retriever.eval()\n",
        "print(\"✓ Vietnamese-SBERT loaded\")\n"
      ],
      "metadata": {
        "id": "p9Pe-duZDiaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 7: CHUNK ATTENTION LAYER - FIXED FOR FP16\n",
        "# ================================================================================\n",
        "\n",
        "class ChunkAttentionLayer(nn.Module):\n",
        "    \"\"\"Attention mechanism ở chunk level - FP16 compatible\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size: int, attention_hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, attention_hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(attention_hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, chunk_embeddings: torch.Tensor, mask: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            chunk_embeddings: [batch, num_chunks, hidden_size]\n",
        "            mask: [batch, num_chunks] - 1 for valid chunks, 0 for padding\n",
        "\n",
        "        Returns:\n",
        "            context: [batch, hidden_size]\n",
        "            attention_weights: [batch, num_chunks]\n",
        "        \"\"\"\n",
        "        scores = self.attention(chunk_embeddings)\n",
        "        scores = scores.squeeze(-1)  # [batch, num_chunks]\n",
        "\n",
        "        if mask is not None:\n",
        "            # FP16 max value is ~65504, so use -65504 instead of -1e9\n",
        "            mask_value = -65504.0 if scores.dtype == torch.float16 else -1e9\n",
        "            scores = scores.masked_fill(mask == 0, mask_value)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=1)\n",
        "\n",
        "        context = torch.bmm(\n",
        "            attention_weights.unsqueeze(1),\n",
        "            chunk_embeddings\n",
        "        ).squeeze(1)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "print(\"✓ ChunkAttentionLayer defined (FP16 compatible)\")"
      ],
      "metadata": {
        "id": "c8C1i4j8DkQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 8: DATASET (FIXED VERSION)\n",
        "# ================================================================================\n",
        "\n",
        "class HierarchicalAttentionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: No empty padding, better RAG retrieval with overlap chunking\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        normalizer: VietnameseTextNormalizer,\n",
        "        retriever: SentenceTransformer,\n",
        "        chunk_size: int = 400,\n",
        "        chunk_overlap: int = 50,\n",
        "        top_k: int = 5,\n",
        "        max_length: int = 256,\n",
        "        min_chunks: int = 3,\n",
        "        min_similarity: float = 0.15,\n",
        "        verbose: bool = False\n",
        "    ):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.normalizer = normalizer\n",
        "        self.retriever = retriever\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.top_k = top_k\n",
        "        self.max_length = max_length\n",
        "        self.min_chunks = min_chunks\n",
        "        self.min_similarity = min_similarity\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Chunker với overlap\n",
        "        self.chunker = SemanticChunkRetriever(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        row = self.df.iloc[idx]\n",
        "        label = int(row['label'])\n",
        "\n",
        "        # 1. Lấy Title và Content\n",
        "        title = str(row.get('title', ''))\n",
        "        content = str(row.get('content', ''))\n",
        "\n",
        "        # Normalize\n",
        "        title = self.normalizer.normalize(title)\n",
        "        content = self.normalizer.normalize(content)\n",
        "\n",
        "        # 2. Chunking Content với overlap\n",
        "        raw_chunks = self.chunker.chunk_document(content)\n",
        "\n",
        "        # Validate minimum chunks\n",
        "        if len(raw_chunks) < self.min_chunks:\n",
        "            if self.verbose:\n",
        "                print(f\"⚠️ Sample {idx}: {len(raw_chunks)} chunks < min {self.min_chunks}\")\n",
        "            while len(raw_chunks) < self.min_chunks:\n",
        "                raw_chunks.extend(raw_chunks[:self.min_chunks - len(raw_chunks)])\n",
        "\n",
        "        # 3. RAG LOGIC: Retrieve Top-K chunks\n",
        "        if len(raw_chunks) <= self.top_k:\n",
        "            selected_chunks = raw_chunks\n",
        "        else:\n",
        "            # Use title as query, fallback to first chunk\n",
        "            query = title if title.strip() else raw_chunks[0]\n",
        "\n",
        "            # Encode query and chunks\n",
        "            query_emb = self.retriever.encode(query, convert_to_tensor=True)\n",
        "            chunk_embs = self.retriever.encode(raw_chunks, convert_to_tensor=True)\n",
        "\n",
        "            # Compute similarities\n",
        "            similarities = F.cosine_similarity(\n",
        "                query_emb.unsqueeze(0),\n",
        "                chunk_embs,\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Filter low-similarity chunks\n",
        "            valid_indices = (similarities >= self.min_similarity).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            if len(valid_indices) < self.top_k:\n",
        "                top_indices = similarities.argsort(descending=True)[:self.top_k]\n",
        "            else:\n",
        "                valid_sims = similarities[valid_indices]\n",
        "                sorted_valid = valid_indices[valid_sims.argsort(descending=True)]\n",
        "                top_indices = sorted_valid[:self.top_k]\n",
        "\n",
        "            selected_chunks = [raw_chunks[i] for i in top_indices]\n",
        "\n",
        "        # Pad nếu thiếu\n",
        "        while len(selected_chunks) < self.top_k:\n",
        "            selected_chunks.append(\n",
        "                selected_chunks[len(selected_chunks) % len(raw_chunks)]\n",
        "            )\n",
        "\n",
        "        selected_chunks = selected_chunks[:self.top_k]\n",
        "\n",
        "        # Validate chunk lengths\n",
        "        max_chars = self.max_length * 4\n",
        "        validated_chunks = []\n",
        "        for chunk in selected_chunks:\n",
        "            if len(chunk) > max_chars:\n",
        "                if self.verbose:\n",
        "                    print(f\"⚠️ Chunk too long ({len(chunk)} chars), truncating\")\n",
        "                chunk = chunk[:max_chars]\n",
        "            validated_chunks.append(chunk)\n",
        "\n",
        "        # Tokenize chunks\n",
        "        chunk_encodings = self.tokenizer(\n",
        "            validated_chunks,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'chunk_input_ids': chunk_encodings['input_ids'],\n",
        "            'chunk_attention_masks': chunk_encodings['attention_mask'],\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"✓ HierarchicalAttentionDataset defined (with overlap)\")\n"
      ],
      "metadata": {
        "id": "zAG-fWPzDle2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 9: HAN MODEL\n",
        "# ================================================================================\n",
        "\n",
        "class HierarchicalAttentionClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical Attention Network cho phân loại fake news\n",
        "\n",
        "    Architecture:\n",
        "    1. PhoBERT encode từng chunk → chunk embeddings\n",
        "    2. Chunk-level attention → document representation\n",
        "    3. Classification head → logits\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        phobert_name: str = \"vinai/phobert-base-v2\",\n",
        "        chunk_attention_hidden: int = 128,\n",
        "        num_classes: int = 2,\n",
        "        dropout: float = 0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # PhoBERT encoder\n",
        "        self.phobert = AutoModel.from_pretrained(phobert_name)\n",
        "        self.hidden_size = self.phobert.config.hidden_size  # 768\n",
        "\n",
        "        # Chunk-level attention\n",
        "        self.chunk_attention = ChunkAttentionLayer(\n",
        "            hidden_size=self.hidden_size,\n",
        "            attention_hidden=chunk_attention_hidden\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.hidden_size // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def encode_chunks(\n",
        "        self,\n",
        "        chunk_input_ids: torch.Tensor,\n",
        "        chunk_attention_masks: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode từng chunk bằng PhoBERT\n",
        "\n",
        "        Args:\n",
        "            chunk_input_ids: [batch, num_chunks, max_length]\n",
        "            chunk_attention_masks: [batch, num_chunks, max_length]\n",
        "\n",
        "        Returns:\n",
        "            chunk_embeddings: [batch, num_chunks, hidden_size]\n",
        "        \"\"\"\n",
        "        batch_size, num_chunks, max_length = chunk_input_ids.shape\n",
        "\n",
        "        # Reshape to [batch * num_chunks, max_length]\n",
        "        flat_input_ids = chunk_input_ids.view(-1, max_length)\n",
        "        flat_attention_masks = chunk_attention_masks.view(-1, max_length)\n",
        "\n",
        "        # Encode with PhoBERT\n",
        "        outputs = self.phobert(\n",
        "            input_ids=flat_input_ids,\n",
        "            attention_mask=flat_attention_masks\n",
        "        )\n",
        "\n",
        "        # Get [CLS] token embeddings: [batch * num_chunks, hidden_size]\n",
        "        chunk_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Reshape back: [batch, num_chunks, hidden_size]\n",
        "        chunk_embeddings = chunk_embeddings.view(batch_size, num_chunks, -1)\n",
        "\n",
        "        return chunk_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        chunk_input_ids: torch.Tensor,\n",
        "        chunk_attention_masks: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            chunk_input_ids: [batch, num_chunks, max_length]\n",
        "            chunk_attention_masks: [batch, num_chunks, max_length]\n",
        "\n",
        "        Returns:\n",
        "            dict with:\n",
        "                - logits: [batch, num_classes]\n",
        "                - chunk_attention: [batch, num_chunks]\n",
        "        \"\"\"\n",
        "        # 1. Encode chunks: [batch, num_chunks, hidden_size]\n",
        "        chunk_embeddings = self.encode_chunks(chunk_input_ids, chunk_attention_masks)\n",
        "\n",
        "        # 2. Create chunk-level mask\n",
        "        chunk_mask = (chunk_attention_masks.sum(dim=2) > 0).float()\n",
        "\n",
        "        # 3. Apply chunk-level attention\n",
        "        doc_representation, chunk_attention_weights = self.chunk_attention(\n",
        "            chunk_embeddings,\n",
        "            mask=chunk_mask\n",
        "        )\n",
        "\n",
        "        # 4. Classification\n",
        "        logits = self.classifier(doc_representation)\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'chunk_attention': chunk_attention_weights\n",
        "        }\n",
        "\n",
        "print(\"✓ HierarchicalAttentionClassifier defined\")\n"
      ],
      "metadata": {
        "id": "OMlBnzozDmw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 10: CREATE DATASETS (UPDATED - THÊM TEST SET)\n",
        "# ================================================================================\n",
        "\n",
        "print(\"Creating training dataset...\")\n",
        "train_dataset = HierarchicalAttentionDataset(\n",
        "    df=train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    normalizer=normalizer,\n",
        "    retriever=retriever,\n",
        "    chunk_size=400,\n",
        "    top_k=5,\n",
        "    max_length=256,\n",
        "    min_chunks=3,\n",
        "    min_similarity=0.15,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"✓ Train dataset: {len(train_dataset)} samples\")\n",
        "\n",
        "print(\"\\nCreating validation dataset...\")\n",
        "val_dataset = HierarchicalAttentionDataset(\n",
        "    df=val_df,\n",
        "    tokenizer=tokenizer,\n",
        "    normalizer=normalizer,\n",
        "    retriever=retriever,\n",
        "    chunk_size=400,\n",
        "    top_k=5,\n",
        "    max_length=256,\n",
        "    min_chunks=3,\n",
        "    min_similarity=0.15,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"✓ Val dataset: {len(val_dataset)} samples\")\n",
        "\n",
        "print(\"\\nCreating test dataset...\")\n",
        "test_dataset = HierarchicalAttentionDataset(\n",
        "    df=test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    normalizer=normalizer,\n",
        "    retriever=retriever,\n",
        "    chunk_size=400,\n",
        "    top_k=5,\n",
        "    max_length=256,\n",
        "    min_chunks=3,\n",
        "    min_similarity=0.15,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"✓ Test dataset: {len(test_dataset)} samples\")\n",
        "\n",
        "# Test one sample\n",
        "print(\"\\n### Testing Dataset ###\")\n",
        "sample = train_dataset[0]\n",
        "print(f\"Sample shape:\")\n",
        "print(f\"  chunk_input_ids: {sample['chunk_input_ids'].shape}\")\n",
        "print(f\"  chunk_attention_masks: {sample['chunk_attention_masks'].shape}\")\n",
        "print(f\"  label: {sample['label']}\")\n",
        "\n",
        "# Check for empty chunks\n",
        "for i in range(5):\n",
        "    num_tokens = (sample['chunk_input_ids'][i] != tokenizer.pad_token_id).sum().item()\n",
        "    print(f\"  Chunk {i}: {num_tokens} tokens\")\n",
        "\n",
        "min_tokens = min(\n",
        "    (sample['chunk_input_ids'][i] != tokenizer.pad_token_id).sum().item()\n",
        "    for i in range(5)\n",
        ")\n",
        "print(f\"\\n{' GOOD' if min_tokens > 10 else ' ISSUE'}: Min tokens = {min_tokens}\")\n"
      ],
      "metadata": {
        "id": "IxJfzshXDoYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 11: TRAINING SETUP WITH CLASS WEIGHTS (OPTIMIZED)\n",
        "# ================================================================================\n",
        "# ========================================\n",
        "\n",
        "# Tính class weights từ training data\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_df['label']),\n",
        "    y=train_df['label'].values\n",
        ")\n",
        "\n",
        "# Convert sang tensor\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "print(f\"Class distribution in training set:\")\n",
        "print(f\"  REAL (0): {(train_df['label'] == 0).sum()} samples\")\n",
        "print(f\"  FAKE (1): {(train_df['label'] == 1).sum()} samples\")\n",
        "print(f\"\\nComputed class weights:\")\n",
        "print(f\"  REAL (0): {class_weights[0]:.4f}\")\n",
        "print(f\"  FAKE (1): {class_weights[1]:.4f}\")\n",
        "print(f\"  Weight ratio (FAKE/REAL): {class_weights[1]/class_weights[0]:.2f}x\")\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# INITIALIZE MODEL\n",
        "# ========================================\n",
        "model = HierarchicalAttentionClassifier(\n",
        "    phobert_name='vinai/phobert-base-v2',\n",
        "    chunk_attention_hidden=128,\n",
        "    num_classes=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\n✓ Model initialized on {device}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# DATALOADERS WITH OPTIMIZED SETTINGS\n",
        "# ========================================\n",
        "use_cuda = torch.cuda.is_available()\n",
        "num_workers = 0  # Must be 0 with CUDA + sentence_transformers\n",
        "\n",
        "if use_cuda:\n",
        "    print(\"\\n CUDA detected: Setting num_workers=0 to avoid multiprocessing errors\")\n",
        "else:\n",
        "    num_workers = 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ DataLoaders created\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "\n",
        "\n",
        "# ========================================\n",
        "# OPTIMIZER & LOSS WITH CLASS WEIGHTS\n",
        "# ========================================\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# WEIGHTED CROSS ENTROPY LOSS\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Training components ready\")\n",
        "print(f\"  Optimizer: AdamW (lr=2e-5)\")\n",
        "print(f\"  Loss: CrossEntropyLoss (WEIGHTED)\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau (patience=2)\")\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "5qkBZ3k7DroU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 12: TRAINING + VALIDATION\n",
        "# ================================================================================\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import gc\n",
        "\n",
        "# Setup\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v4.1'\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = GradScaler() if use_amp else None\n",
        "accumulation_steps = 4  # Tăng lên 4 để giảm VRAM\n",
        "best_val_f1 = 0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "num_epochs = 15\n",
        "\n",
        "history = {'train_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': [],\n",
        "           'val_precision': [], 'val_recall': []}\n",
        "\n",
        "print(f\"✓ Save: {drive_save_dir}\")\n",
        "if use_amp:\n",
        "    print(f\"⚡ FP16 + Accum={accumulation_steps} (effective batch={16*accumulation_steps})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"TRAINING ({num_epochs} epochs)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*80}\\nEPOCH {epoch+1}/{num_epochs}\\n{'='*80}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}\")\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        ids = batch['chunk_input_ids'].to(device, non_blocking=True)\n",
        "        masks = batch['chunk_attention_masks'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with autocast(dtype=torch.float16):\n",
        "                logits = model(ids, masks)['logits']  # Tách ra để save memory\n",
        "                loss = criterion(logits, labels) / accumulation_steps\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            logits = model(ids, masks)['logits']\n",
        "            loss = criterion(logits, labels) / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "        # Free memory after backward\n",
        "        del ids, masks, logits\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            if use_amp:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # Metrics without extra forward pass\n",
        "        with torch.no_grad():\n",
        "            preds = torch.argmax(criterion.weight.new_zeros(labels.size(0), 2).copy_(\n",
        "                model(batch['chunk_input_ids'].to(device),\n",
        "                      batch['chunk_attention_masks'].to(device))['logits']), dim=1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{loss.item()*accumulation_steps:.4f}',\n",
        "                         'acc': f'{train_correct/train_total:.4f}'})\n",
        "\n",
        "        # More aggressive cache clearing\n",
        "        if batch_idx % 20 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # Clear before validation\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            ids = batch['chunk_input_ids'].to(device, non_blocking=True)\n",
        "            masks = batch['chunk_attention_masks'].to(device, non_blocking=True)\n",
        "            labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "            if use_amp:\n",
        "                with autocast(dtype=torch.float16):\n",
        "                    logits = model(ids, masks)['logits']\n",
        "            else:\n",
        "                logits = model(ids, masks)['logits']\n",
        "\n",
        "            val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Free immediately\n",
        "            del ids, masks, logits\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    val_precision = precision_score(val_labels, val_preds, average='macro')\n",
        "    val_recall = recall_score(val_labels, val_preds, average='macro')\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    history['val_precision'].append(val_precision)\n",
        "    history['val_recall'].append(val_recall)\n",
        "\n",
        "    print(f\"\\n### EPOCH {epoch+1} ###\")\n",
        "    print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
        "    print(f\"Val:   Acc={val_acc:.4f}, F1={val_f1:.4f}, P={val_precision:.4f}, R={val_recall:.4f}\")\n",
        "\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        improvement = val_f1 - best_val_f1\n",
        "        best_val_f1 = val_f1\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Move to CPU before saving\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': {k: v.cpu() for k, v in model.state_dict().items()},\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict() if use_amp else None,\n",
        "            'val_f1': val_f1,\n",
        "            'val_acc': val_acc,\n",
        "            'history': history\n",
        "        }\n",
        "        torch.save(checkpoint, best_model_path)\n",
        "        # Move back to GPU\n",
        "        model.load_state_dict({k: v.to(device) for k, v in checkpoint['model_state_dict'].items()})\n",
        "\n",
        "        print(f\"Best saved! F1={best_val_f1:.4f} (↑{improvement:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improve. Patience: {patience_counter}/{patience}\")\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stop at epoch {epoch+1}, Best F1={best_val_f1:.4f}\")\n",
        "            break\n",
        "\n",
        "    # Aggressive cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING DONE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Val F1: {best_val_f1:.4f} (Epoch {history['val_f1'].index(max(history['val_f1']))+1})\")\n",
        "\n",
        "# Visualize training\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "axes[0,0].plot(history['train_loss'], 'o-', label='Train Loss')\n",
        "axes[0,0].set_title('Loss')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(alpha=0.3)\n",
        "\n",
        "axes[0,1].plot(history['train_acc'], 'o-', label='Train')\n",
        "axes[0,1].plot(history['val_acc'], 's-', label='Val')\n",
        "axes[0,1].set_title('Accuracy')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(alpha=0.3)\n",
        "\n",
        "axes[1,0].plot(history['val_f1'], 'o-', color='green', label='Val F1')\n",
        "best_epoch = history['val_f1'].index(max(history['val_f1']))\n",
        "axes[1,0].scatter(best_epoch, max(history['val_f1']), color='red', s=100, label='Best', zorder=5)\n",
        "axes[1,0].set_title('F1 Score')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(alpha=0.3)\n",
        "\n",
        "axes[1,1].plot(history['val_precision'], 'o-', label='Precision')\n",
        "axes[1,1].plot(history['val_recall'], 's-', label='Recall')\n",
        "axes[1,1].set_title('Precision & Recall')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(drive_save_dir, 'training.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Final validation\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "val_preds = []\n",
        "val_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Final Validation\"):\n",
        "        ids = batch['chunk_input_ids'].to(device, non_blocking=True)\n",
        "        masks = batch['chunk_attention_masks'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with autocast(dtype=torch.float16):\n",
        "                logits = model(ids, masks)['logits']\n",
        "        else:\n",
        "            logits = model(ids, masks)['logits']\n",
        "\n",
        "        val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDATION (BEST MODEL)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:   {accuracy_score(val_labels, val_preds):.4f}\")\n",
        "print(f\"F1:         {f1_score(val_labels, val_preds, average='macro'):.4f}\")\n",
        "print(f\"Precision:  {precision_score(val_labels, val_preds, average='macro'):.4f}\")\n",
        "print(f\"Recall:     {recall_score(val_labels, val_preds, average='macro'):.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"[[{cm[0,0]:4d} {cm[0,1]:4d}]  (REAL)\")\n",
        "print(f\" [{cm[1,0]:4d} {cm[1,1]:4d}]] (FAKE)\")"
      ],
      "metadata": {
        "id": "JETbfnInDt33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 13: TEST EVALUATION\n",
        "# ================================================================================\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Testing model from epoch {checkpoint['epoch']} (Val F1={checkpoint['val_f1']:.4f})\")\n",
        "\n",
        "# Test\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "test_probs = []\n",
        "test_attention = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        ids = batch['chunk_input_ids'].to(device, non_blocking=True)\n",
        "        masks = batch['chunk_attention_masks'].to(device, non_blocking=True)\n",
        "        labels = batch['label'].to(device, non_blocking=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with autocast():\n",
        "                outputs = model(ids, masks)\n",
        "        else:\n",
        "            outputs = model(ids, masks)\n",
        "\n",
        "        logits = outputs['logits']\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        test_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "        test_probs.extend(probs.cpu().numpy())\n",
        "        test_attention.extend(outputs['chunk_attention'].cpu().numpy())\n",
        "\n",
        "# Metrics\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
        "test_f1_weighted = f1_score(test_labels, test_preds, average='weighted')\n",
        "test_precision = precision_score(test_labels, test_preds, average='macro')\n",
        "test_recall = recall_score(test_labels, test_preds, average='macro')\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:     {test_acc:.4f}\")\n",
        "print(f\"F1 (Macro):   {test_f1:.4f}\")\n",
        "print(f\"F1 (Weight):  {test_f1_weighted:.4f}\")\n",
        "print(f\"Precision:    {test_precision:.4f}\")\n",
        "print(f\"Recall:       {test_recall:.4f}\")\n",
        "\n",
        "print(\"\\n\" + classification_report(test_labels, test_preds, target_names=['REAL', 'FAKE']))\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"[[{cm[0,0]:4d} {cm[0,1]:4d}]\")\n",
        "print(f\" [{cm[1,0]:4d} {cm[1,1]:4d}]]\")\n",
        "print(f\"\\nTN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}\")\n",
        "\n",
        "# Attention analysis\n",
        "mean_attn = np.array(test_attention).mean(axis=0)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ATTENTION WEIGHTS\")\n",
        "print(\"=\"*80)\n",
        "for i, w in enumerate(mean_attn):\n",
        "    bar = \"█\" * int(w * 40 / mean_attn.max())\n",
        "    print(f\"Chunk {i}: {bar} {w:.4f}\")\n",
        "\n",
        "# Val-Test comparison\n",
        "gap = abs(checkpoint['val_f1'] - test_f1)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VAL vs TEST\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Val F1:  {checkpoint['val_f1']:.4f}\")\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"Gap:     {gap:.4f} {'Good' if gap < 0.02 else ' Overfitting' if gap < 0.05 else ' Bad'}\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['REAL', 'FAKE'], yticklabels=['REAL', 'FAKE'], ax=axes[0])\n",
        "axes[0].set_title('Confusion Matrix')\n",
        "axes[0].set_ylabel('True')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "\n",
        "axes[1].bar(range(5), mean_attn, color='skyblue', edgecolor='navy')\n",
        "axes[1].set_title('Attention Weights')\n",
        "axes[1].set_xlabel('Chunk')\n",
        "axes[1].set_ylabel('Weight')\n",
        "for i, v in enumerate(mean_attn):\n",
        "    axes[1].text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(drive_save_dir, 'test_results.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTest complete. Results saved to {drive_save_dir}\")"
      ],
      "metadata": {
        "id": "AIz-OzXe4d7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# EXPORT TOKENIZER & ONNX - SIMPLE VERSION\n",
        "# ================================================================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import onnx\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPORT TOKENIZER & ONNX MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Paths\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v4.1'\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "\n",
        "# ========================================\n",
        "# 2. EXPORT ONNX (Single batch only)\n",
        "# ========================================\n",
        "print(\"\\n2. Exporting ONNX model...\")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to('cpu')\n",
        "model.eval()\n",
        "print(f\"✓ Loaded model from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "# Export with FIXED batch=1 (no dynamic)\n",
        "onnx_path = os.path.join(export_dir, 'han_rag_model.onnx')\n",
        "dummy_input = (\n",
        "    torch.randint(0, tokenizer.vocab_size, (1, 5, 256), dtype=torch.long),\n",
        "    torch.ones((1, 5, 256), dtype=torch.long)\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=14,  # Safe version\n",
        "    do_constant_folding=True,\n",
        "    input_names=['chunk_input_ids', 'chunk_attention_masks'],\n",
        "    output_names=['logits', 'chunk_attention']\n",
        ")\n",
        "\n",
        "# Verify\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "onnx_size = os.path.getsize(onnx_path) / (1024**2)\n",
        "print(f\"✓ ONNX saved: {onnx_path}\")\n",
        "print(f\"  Size: {onnx_size:.2f} MB\")\n",
        "\n",
        "# ========================================\n",
        "# 3. SUMMARY\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPORT COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFiles exported:\")\n"
      ],
      "metadata": {
        "id": "3Vuyw8iBHKSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After export\n",
        "print(\"\\n3. Creating external data file...\")\n",
        "\n",
        "# Load and save with external data\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.save_model(\n",
        "    onnx_model,\n",
        "    onnx_path,\n",
        "    save_as_external_data=True,\n",
        "    all_tensors_to_one_file=True,\n",
        "    location='han_rag_model.onnx.data',\n",
        "    size_threshold=1024  # Save tensors >1KB externally\n",
        ")\n",
        "\n",
        "print(f\"✓ External data created: han_rag_model.onnx.data\")\n"
      ],
      "metadata": {
        "id": "EJ5K3rBIj1qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CELL 19: TEST ONNX MODEL WITH TEXT INPUT (NEW)\n",
        "# ================================================================================\n",
        "\n",
        "def predict_with_onnx(\n",
        "    title: str,\n",
        "    content: str,\n",
        "    ort_session: ort.InferenceSession,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    retriever,\n",
        "    device='gpu'\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Predict using ONNX model with raw text input\n",
        "    \"\"\"\n",
        "    # Create mini dataset\n",
        "    test_df = pd.DataFrame({\n",
        "        'title': [title],\n",
        "        'content': [content],\n",
        "        'label': [0]  # Dummy\n",
        "    })\n",
        "\n",
        "    test_dataset = HierarchicalAttentionDataset(\n",
        "        df=test_df,\n",
        "        tokenizer=tokenizer,\n",
        "        normalizer=normalizer,\n",
        "        retriever=retriever,\n",
        "        chunk_size=400,\n",
        "        top_k=5,\n",
        "        max_length=256,\n",
        "        min_chunks=3\n",
        "    )\n",
        "\n",
        "    sample = test_dataset[0]\n",
        "\n",
        "    # Prepare ONNX inputs\n",
        "    onnx_inputs = {\n",
        "        'chunk_input_ids': sample['chunk_input_ids'].unsqueeze(0).numpy(),\n",
        "        'chunk_attention_masks': sample['chunk_attention_masks'].unsqueeze(0).numpy()\n",
        "    }\n",
        "\n",
        "    # Run inference\n",
        "    onnx_outputs = ort_session.run(None, onnx_inputs)\n",
        "    logits = onnx_outputs[0][0]  # [2]\n",
        "    chunk_attention = onnx_outputs[1][0]  # [5]\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "    pred_class = np.argmax(probs)\n",
        "\n",
        "    return {\n",
        "        'prediction': 'FAKE' if pred_class == 1 else 'REAL',\n",
        "        'confidence': float(probs[pred_class]),\n",
        "        'probabilities': {\n",
        "            'REAL': float(probs[0]),\n",
        "            'FAKE': float(probs[1])\n",
        "        },\n",
        "        'chunk_attention': chunk_attention.tolist()\n",
        "    }\n",
        "\n",
        "\n",
        "# Test với COVID-19 example\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING ONNX MODEL WITH TEXT INPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_title = \"NÓNG: Thế giới đối mặt với COVID-19\"\n",
        "test_content = \"\"\"NÓNG ! 15/3 sẽ là ngày đáng nhớ cho không chỉ Việt Nam mà của toàn thế giới do những gì COVID-19 gây ra !!! - Rạng sáng 15/3, Pháp ra lệnh đóng cửa toàn bộ nhà hàng, rạp chiếu phim, cửa hàng,..trừ.. siêu thị, trạm xăng, ngân hàng, tabac, presse báo chí và pharmacie.\"\"\"\n",
        "\n",
        "print(f\"\\nInput:\")\n",
        "print(f\"  Title: {test_title}\")\n",
        "print(f\"  Content: {test_content[:150]}...\")\n",
        "\n",
        "# Predict\n",
        "result = predict_with_onnx(\n",
        "    test_title,\n",
        "    test_content,\n",
        "    ort_session,\n",
        "    tokenizer,\n",
        "    normalizer,\n",
        "    retriever\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ONNX PREDICTION RESULT\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Prediction: {result['prediction']}\")\n",
        "print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "print(f\"\\nProbabilities:\")\n",
        "print(f\"  REAL: {result['probabilities']['REAL']:.4f}\")\n",
        "print(f\"  FAKE: {result['probabilities']['FAKE']:.4f}\")\n",
        "\n",
        "print(f\"\\nChunk Attention Weights:\")\n",
        "for i, weight in enumerate(result['chunk_attention']):\n",
        "    bar_length = int(weight * 40 / max(result['chunk_attention']))\n",
        "    bar = \"█\" * bar_length\n",
        "    print(f\"  Chunk {i}: {bar} {weight:.4f}\")\n",
        "\n",
        "print(\"\\nONNX model inference completed successfully!\")\n"
      ],
      "metadata": {
        "id": "9qKaa5XAHXUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# CONFIG & PATHS\n",
        "# ================================================================================\n",
        "drive_save_dir = '/content/drive/MyDrive/FakeNewsModels/model_v4.1'\n",
        "best_model_path = os.path.join(drive_save_dir, 'han_rag_best.pth')\n",
        "onnx_path = os.path.join(drive_save_dir, 'han_rag_model.onnx')\n",
        "deployment_dir = os.path.join(drive_save_dir, 'deployment_package')\n",
        "\n",
        "batch_size = 1\n",
        "num_chunks = 5\n",
        "max_length = 256\n",
        "\n",
        "# ================================================================================\n",
        "# LOAD DEPENDENCIES\n",
        "# ================================================================================\n",
        "try:\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "except ImportError:\n",
        "    !pip install -q onnx onnxruntime\n",
        "    import onnx\n",
        "    import onnxruntime as ort  # [web:21][web:30]\n",
        "\n",
        "# ================================================================================\n",
        "# LOAD MODEL & DUMMY INPUT\n",
        "# ================================================================================\n",
        "checkpoint = torch.load(best_model_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "dummy_chunk_input_ids = torch.randint(\n",
        "    0, tokenizer.vocab_size,\n",
        "    (batch_size, num_chunks, max_length),\n",
        "    dtype=torch.long, device=device\n",
        ")\n",
        "dummy_chunk_attention_masks = torch.ones(\n",
        "    (batch_size, num_chunks, max_length),\n",
        "    dtype=torch.long, device=device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = model(dummy_chunk_input_ids, dummy_chunk_attention_masks)\n",
        "\n",
        "# ================================================================================\n",
        "# EXPORT TO ONNX\n",
        "# ================================================================================\n",
        "input_names = ['chunk_input_ids', 'chunk_attention_masks']\n",
        "output_names = ['logits', 'chunk_attention']\n",
        "dynamic_axes = {\n",
        "    'chunk_input_ids': {0: 'batch_size'},\n",
        "    'chunk_attention_masks': {0: 'batch_size'},\n",
        "    'logits': {0: 'batch_size'},\n",
        "    'chunk_attention': {0: 'batch_size'}\n",
        "}\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (dummy_chunk_input_ids, dummy_chunk_attention_masks),\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=14,\n",
        "    do_constant_folding=True,\n",
        "    input_names=input_names,\n",
        "    output_names=output_names,\n",
        "    dynamic_axes=dynamic_axes,\n",
        "    verbose=False\n",
        ")  # [web:21][web:30]\n",
        "\n",
        "onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "\n",
        "# ================================================================================\n",
        "# VALIDATE & QUICK TEST ONNX\n",
        "# ================================================================================\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "ort_session = ort.InferenceSession(\n",
        "    onnx_path,\n",
        "    providers=['CPUExecutionProvider']\n",
        ")\n",
        "\n",
        "test_sample = test_dataset[0]\n",
        "test_input_ids = test_sample['chunk_input_ids'].unsqueeze(0).numpy()\n",
        "test_attention_masks = test_sample['chunk_attention_masks'].unsqueeze(0).numpy()\n",
        "\n",
        "onnx_inputs = {\n",
        "    'chunk_input_ids': test_input_ids,\n",
        "    'chunk_attention_masks': test_attention_masks\n",
        "}\n",
        "onnx_logits, onnx_attention = ort_session.run(None, onnx_inputs)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pt_out = model(\n",
        "        test_sample['chunk_input_ids'].unsqueeze(0).to(device),\n",
        "        test_sample['chunk_attention_masks'].unsqueeze(0).to(device)\n",
        "    )\n",
        "    pytorch_logits = pt_out['logits'].cpu().numpy()\n",
        "    pytorch_attention = pt_out['chunk_attention'].cpu().numpy()\n",
        "\n",
        "logits_diff = float(np.abs(onnx_logits - pytorch_logits).max())\n",
        "attention_diff = float(np.abs(onnx_attention - pytorch_attention).max())\n",
        "\n",
        "pytorch_pred = int(np.argmax(pytorch_logits, axis=1)[0])\n",
        "onnx_pred = int(np.argmax(onnx_logits, axis=1)[0])\n",
        "pred_match = bool(pytorch_pred == onnx_pred)\n",
        "\n",
        "# ================================================================================\n",
        "# SAVE ONNX METADATA\n",
        "# ================================================================================\n",
        "onnx_metadata = {\n",
        "    'model_type': 'HierarchicalAttentionClassifier',\n",
        "    'framework': 'PyTorch -> ONNX',\n",
        "    'opset_version': 14,\n",
        "    'input_shapes': {\n",
        "        'chunk_input_ids': [batch_size, num_chunks, max_length],\n",
        "        'chunk_attention_masks': [batch_size, num_chunks, max_length]\n",
        "    },\n",
        "    'output_shapes': {\n",
        "        'logits': [batch_size, 2],\n",
        "        'chunk_attention': [batch_size, num_chunks]\n",
        "    },\n",
        "    'model_config': {\n",
        "        'phobert_name': 'vinai/phobert-base-v2',\n",
        "        'chunk_attention_hidden': 128,\n",
        "        'num_classes': 2,\n",
        "        'dropout': 0.3,\n",
        "        'chunk_size': 400,\n",
        "        'top_k': 5,\n",
        "        'max_length': 256\n",
        "    },\n",
        "    'training_info': {\n",
        "        'best_epoch': checkpoint['epoch'],\n",
        "        'val_f1': float(checkpoint['val_f1']),\n",
        "        'val_acc': float(checkpoint.get('val_acc', 0))\n",
        "    },\n",
        "    'validation': {\n",
        "        'max_logits_diff': logits_diff,\n",
        "        'max_attention_diff': attention_diff,\n",
        "        'predictions_match': pred_match\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(drive_save_dir, 'onnx_metadata.json')\n",
        "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(onnx_metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# ================================================================================\n",
        "# DEPLOYMENT PACKAGE (TOKENIZER, RETRIEVER, ONNX, CONFIG) + ZIP\n",
        "# ================================================================================\n",
        "os.makedirs(deployment_dir, exist_ok=True)\n",
        "\n",
        "# 1. Tokenizer\n",
        "tokenizer_dir = os.path.join(deployment_dir, 'tokenizer')\n",
        "tokenizer.save_pretrained(tokenizer_dir)  # [web:16][web:13]\n",
        "\n",
        "# 2. Retriever\n",
        "retriever_dir = os.path.join(deployment_dir, 'retriever')\n",
        "retriever.save(retriever_dir)\n",
        "\n",
        "# 3. ONNX\n",
        "shutil.copy(onnx_path, os.path.join(deployment_dir, 'han_rag_model.onnx'))\n",
        "\n",
        "# 4. Config cho deployment\n",
        "config = {\n",
        "    'model_version': 'v2',\n",
        "    'best_epoch': checkpoint['epoch'],\n",
        "    'val_f1': float(checkpoint['val_f1']),\n",
        "    'model_config': {\n",
        "        'chunk_size': 400,\n",
        "        'top_k': 5,\n",
        "        'max_length': 256\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(deployment_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# 5. ZIP\n",
        "zip_base = os.path.join(drive_save_dir, 'deployment_package')\n",
        "shutil.make_archive(zip_base, 'zip', deployment_dir)  # [web:24][web:36]\n",
        "zip_path = zip_base + '.zip'\n",
        "zip_size = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "\n",
        "print(f\"ONNX: {onnx_path} ({onnx_size_mb:.2f} MB)\")\n",
        "print(f\"Diff logits: {logits_diff:.6f}, attention: {attention_diff:.6f}, match: {pred_match}\")\n",
        "print(f\"Metadata: {metadata_path}\")\n",
        "print(f\"Deployment ZIP: {zip_path} ({zip_size:.2f} MB)\")\n",
        "print(\"Contents: han_rag_model.onnx, tokenizer/, retriever/, config.json\")"
      ],
      "metadata": {
        "id": "91Jp4jwb5MSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}